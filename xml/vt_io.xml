<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE sect1
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<sect1 xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="sec.vt.io">
 <title>I/O Virtualization</title>

 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker>
   </dm:bugtracker>
  </dm:docmanager>
 </info>
 <para>
  &vmguest;s not only share CPU and memory resources of the host system,
  but also the I/O subsystem. Because software I/O virtualization techniques
  deliver less performance than bare metal, hardware solutions that deliver
  almost <quote>native</quote> performance have been developed recently.
  &productname; supports the following I/O virtualization techniques:
 </para>

 <variablelist>
  <varlistentry xml:id="vt.io.fullv">
   <term>Full Virtualization</term>
   <listitem>
    <para>
     Fully Virtualized (FV) drivers emulate widely supported real devices,
     which can be used with an existing driver in the &vmguest;. Since
     the physical device on the &vmhost; may differ from the emulated
     one, the hypervisor needs to process all I/O operations before handing
     them over to the physical device. Therefore all I/O operations need to
     traverse two software layers, a process that not only significantly
     impacts I/O performance, but also consumes CPU time.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry xml:id="vt.io.parav">
   <term>Paravirtualization</term>
   <listitem>
    <para>
     Paravirtualization (PV) allows direct communication between the
     hypervisor and the &vmguest;. With less overhead involved,
     performance is much better than with full virtualization. However,
     paravirtualization requires either the guest operating system to be
     modified to support the paravirtualization API or paravirtualized
     drivers. See <xref linkend="sec.kvm.requires.guests.virt_drivers"/> for
     a list of guest operating systems supporting paravirtualization.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry xml:id="vt.io.vfio">
   <term>VFIO</term>
   <listitem>
    <para>
     VFIO stands for <emphasis>Virtual Function I/O</emphasis> and is a new
     user-level driver framework for Linux. It replaces the traditional &kvm;
     &pciback; device assignment. The VFIO driver exposes direct device access
     to userspace in a secure memory (<xref
     linkend="gloss.vt.acronym.iommu"/>) protected environment. With VFIO, a
     &vmguest; can directly access hardware devices on the &vmhost;
     (pass-through), avoiding performance issues caused by emulation in
     performance critical paths. This method does not allow to share
     devices&mdash;each device can only be assigned to a single
     &vmguest;. VFIO needs to be supported by the &vmhost; CPU, chipset and
     the BIOS/EFI.
    </para>
    <para>
     Compared to the legacy &kvm; PCI device assignment, VFIO has the
     following advantages:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       Resource access is compatible with secure boot.
      </para>
     </listitem>
     <listitem>
      <para>
       Device is isolated and its memory access protected.
      </para>
     </listitem>
     <listitem>
      <para>
       Offers a userspace device driver with more flexible device ownership
       model.
      </para>
     </listitem>
     <listitem>
      <para>
       Is independent of &kvm; technology, and not bound to x86
       architecture only.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </varlistentry>
  <varlistentry xml:id="vt.io.sriov">
   <term>SR-IOV</term>
   <listitem>
    <para>
     The latest I/O virtualization technique, Single Root I/O Virtualization
     SR-IOV combines the benefits of the aforementioned
     techniques&mdash;performance and the ability to share a device with
     several &vmguest;s. SR-IOV requires special I/O devices, that are capable
     of replicating resources so they appear as multiple separate
     devices. Each such <quote>pseudo</quote> device can be directly used by a
     single guest. However, for network cards for example the number of
     concurrent queues that can be used is limited, potentially reducing
     performance for the &vmguest; compared to paravirtualized drivers.  On
     the &vmhost;, SR-IOV must be supported by the I/O device, the CPU and
     chipset, the BIOS/EFI and the hypervisor&mdash;for setup instructions see
     <xref linkend="sec.libvirt.config.pci"/>.
<!-- fs 2014-02-21: no list available
     ATM &productname; supports the SRV-IOV capable network cards listed below
     -->
    </para>
   </listitem>
  </varlistentry>

  <varlistentry xml:id="vt.io.pcipass">
   <term>&kvm; &pciback; (deprecated)</term>
   <listitem>
    <para>
     This method of assigning PCI devices to &vmguest;s is deprecated and has
     been replaced by VFIO. &kvm; &pciback; is still supported by &suse;, but
     using VFIO instead is strongly recommended. Support for &kvm; &pciback;
     will be removed from future versions of &productname;.
    </para>
   </listitem>
   </varlistentry>
 </variablelist>

 <important xml:id="ann.vt.io.require">
  <title>Requirements for VFIO and SR-IOV</title>
  <para>
   To be able to use the VFIO and SR-IOV features, the &vmhost; needs to
   fulfill the following requirements:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     IOMMU needs to be enabled in the BIOS/EFI.
    </para>
   </listitem>
   <listitem>
    <para>
     For Intel CPUs, the Kernel parameter <literal>intel_iommu=on</literal>
     needs to be provided on the Kernel command line. Refer to <xref
     linkend="sec.grub2.yast2.config.advanced.tab2"/> for details.
    </para>
   </listitem>
   <listitem>
    <para>
     The VFIO infrastructure needs to be available. This can be achieved by
     loading the Kernel module <systemitem
     class="resource">vfio_pci</systemitem>. Refer to <xref
     linkend="sec.boot.systemd.advanced.kernel_modules"/> for details.
    </para>
   </listitem>
  </itemizedlist>
 </important>

</sect1>
