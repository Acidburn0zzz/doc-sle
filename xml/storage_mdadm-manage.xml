<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd" [
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter conformance="sles11,Novell,yes,,80" id="raidmdadm" lang="en" revision="01/23/09">
  <title>Managing Software RAIDs 6 and 10 with mdadm</title>
  <para>
   This section describes how to create software RAID 6 and 10 devices,
   using the Multiple Devices Administration (<command>mdadm(8)</command>)
   tool. You can also use <filename>mdadm</filename> to create RAIDs 0, 1,
   4, and 5. The <command>mdadm</command> tool provides the functionality of
   legacy programs <command>mdtools</command> and
   <command>raidtools</command>.
  </para>
  <itemizedlist role="subtoc">
   <listitem>
    <para>
     <xref linkend="raidmdadmr6" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="raidmdadmr10nest" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="raidmdadmr10cpx" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="raidmdadmdegraded" xrefstyle="SectTitleOnPage"/>
    </para>
   </listitem>
  </itemizedlist>
  <sect1 id="raidmdadmr6">
   <title>Creating a RAID 6</title>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="raidmdadmr6ovw" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="raidmdadmr6create" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="raidmdadmr6ovw">
    <title>Understanding RAID 6</title>
    <para>
     RAID 6 is essentially an extension of RAID 5 that allows for additional
     fault tolerance by using a second independent distributed parity scheme
     (dual parity). Even if two of the hard disk drives fail during the data
     recovery process, the system continues to be operational, with no data
     loss.
    </para>
    <para>
     RAID 6 provides for extremely high data fault tolerance by sustaining
     multiple simultaneous drive failures. It handles the loss of any two
     devices without data loss. Accordingly, it requires N+2 drives to store
     N drives worth of data. It requires a minimum of 4 devices.
    </para>
    <para>
     The performance for RAID 6 is slightly lower but comparable to RAID 5
     in normal mode and single disk failure mode. It is very slow in dual
     disk failure mode.
    </para>
    <table id="b8i9zn7" frame="topbot" rowsep="1" pgwide="0">
     <title>Comparison of RAID 5 and RAID 6</title>
     <tgroup cols="3">
      <colspec colnum="1" colname="1" colwidth="3334*"/>
      <colspec colnum="2" colname="2" colwidth="3334*"/>
      <colspec colnum="3" colname="3" colwidth="3334*"/>
      <thead>
       <row id="b8ihegi">
        <entry>
         <para>
          Feature
         </para>
        </entry>
        <entry>
         <para>
          RAID 5
         </para>
        </entry>
        <entry>
         <para>
          RAID 6
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b8ihegj">
        <entry>
         <para>
          Number of devices
         </para>
        </entry>
        <entry>
         <para>
          N+1, minimum of 3
         </para>
        </entry>
        <entry>
         <para>
          N+2, minimum of 4
         </para>
        </entry>
       </row>
       <row id="b8ihegk">
        <entry>
         <para>
          Parity
         </para>
        </entry>
        <entry>
         <para>
          Distributed, single
         </para>
        </entry>
        <entry>
         <para>
          Distributed, dual
         </para>
        </entry>
       </row>
       <row id="b8ihegl">
        <entry>
         <para>
          Performance
         </para>
        </entry>
        <entry>
         <para>
          Medium impact on write and rebuild
         </para>
        </entry>
        <entry>
         <para>
          More impact on sequential write than RAID 5
         </para>
        </entry>
       </row>
       <row id="b8ihegm">
        <entry>
         <para>
          Fault-tolerance
         </para>
        </entry>
        <entry>
         <para>
          Failure of one component device
         </para>
        </entry>
        <entry>
         <para>
          Failure of two component devices
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   </sect2>

   <sect2 id="raidmdadmr6create">
    <title>Creating a RAID 6</title>
    <para>
     The procedure in this section creates a RAID 6 device
     <filename>/dev/md0</filename> with four devices:
     <filename>/dev/sda1</filename>, <filename>/dev/sdb1</filename>,
     <filename>/dev/sdc1</filename>, and <filename>/dev/sdd1</filename>.
     Ensure that you modify the procedure to use your actual device nodes.
    </para>
    <procedure id="b8ihegn">
     <step id="b8ihego">
      <para>
       Open a terminal console, then log in as the
       <systemitem>root</systemitem> user or equivalent.
      </para>
     </step>
     <step id="b8ihegp">
      <para>
       Create a RAID 6 device. At the command prompt, enter
      </para>
<screen>
mdadm --create /dev/md0 --run --level=raid6 --chunk=128 --raid-devices=4 /dev/sdb1 /dev/sdc1 /dev/sdc1 /dev/sdd1
</screen>
      <para>
       The default chunk size is 64 KB.
      </para>
     </step>
     <step id="b8ihegq">
      <para>
       Create a file system on the RAID 6 device
       <filename>/dev/md0</filename>, such as a Reiser file system
       (reiserfs). For example, at the command prompt, enter
      </para>
<screen>
mkfs.reiserfs /dev/md0
</screen>
      <para>
       Modify the command if you want to use a different file system.
      </para>
     </step>
     <step id="b8ihegr">
      <para>
       Edit the <filename>/etc/mdadm.conf</filename> file to add entries for
       the component devices and the RAID device
       <filename>/dev/md0</filename>.
      </para>
     </step>
     <step id="b8ihegs">
      <para>
       Edit the <filename>/etc/fstab</filename> file to add an entry for the
       RAID 6 device <filename>/dev/md0</filename>.
      </para>
     </step>
     <step id="b8ihegt">
      <para>
       Reboot the server.
      </para>
      <para>
       The RAID 6 device is mounted to <filename>/local</filename>.
      </para>
     </step>
     <step id="b8ihegu">
      <para>
       (Optional) Add a hot spare to service the RAID array. For example, at
       the command prompt enter:
      </para>
<screen>
mdadm /dev/md0 -a /dev/sde1
</screen>
     </step>
    </procedure>
   </sect2>
  </sect1>
  <sect1 id="raidmdadmr10nest">
   <title>Creating Nested RAID 10 Devices with mdadm</title>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="raidmdadmr10nestovw" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="raidmdadmr10nest1" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="raidmdadmr10nest2" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="raidmdadmr10nestovw">
    <title>Understanding Nested RAID Devices</title>
    <para>
     A nested RAID device consists of a RAID array that uses another RAID
     array as its basic element, instead of using physical disks. The goal
     of this configuration is to improve the performance and fault tolerance
     of the RAID.
    </para>
    <para>
     Linux supports nesting of RAID 1 (mirroring) and RAID 0 (striping)
     arrays. Generally, this combination is referred to as RAID 10. To
     distinguish the order of the nesting, this document uses the following
     terminology:
    </para>
    <itemizedlist>
     <listitem>
      <formalpara id="b8ghlom">
       <title>RAID 1+0:</title>
       <para>
        RAID 1 (mirror) arrays are built first, then combined to form a RAID
        0 (stripe) array.
       </para>
      </formalpara>
     </listitem>
     <listitem>
      <formalpara id="b8gzg7q">
       <title>RAID 0+1:</title>
       <para>
        RAID 0 (stripe) arrays are built first, then combined to form a RAID
        1 (mirror) array.
       </para>
      </formalpara>
     </listitem>
    </itemizedlist>
    <para>
     The following table describes the advantages and disadvantages of RAID
     10 nesting as 1+0 versus 0+1. It assumes that the storage objects you
     use reside on different disks, each with a dedicated I/O capability.
    </para>
    <table id="b57a7uv" frame="topbot" rowsep="1" pgwide="0">
     <title>Nested RAID Levels</title>
     <tgroup cols="3">
      <colspec colnum="1" colname="1" colwidth="1191*"/>
      <colspec colnum="2" colname="2" colwidth="1905*"/>
      <colspec colnum="3" colname="3" colwidth="6906*"/>
      <thead>
       <row id="b57a7uw">
        <entry>
         <para>
          RAID Level
         </para>
        </entry>
        <entry>
         <para>
          Description
         </para>
        </entry>
        <entry>
         <para>
          Performance and Fault Tolerance
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b57a7uz">
        <entry>
         <para>
          10 (1+0)
         </para>
        </entry>
        <entry>
         <para>
          RAID 0 (stripe) built with RAID 1 (mirror) arrays
         </para>
        </entry>
        <entry>
         <para>
          RAID 1+0 provides high levels of I/O performance, data redundancy,
          and disk fault tolerance. Because each member device in the RAID 0
          is mirrored individually, multiple disk failures can be tolerated
          and data remains available as long as the disks that fail are in
          different mirrors.
         </para>
         <para>
          You can optionally configure a spare for each underlying mirrored
          array, or configure a spare to serve a spare group that serves all
          mirrors.
         </para>
        </entry>
       </row>
       <row id="b57a7uy">
        <entry>
         <para>
          10 (0+1)
         </para>
        </entry>
        <entry>
         <para>
          RAID 1 (mirror) built with RAID 0 (stripe) arrays
         </para>
        </entry>
        <entry>
         <para>
          RAID 0+1 provides high levels of I/O performance and data
          redundancy, but slightly less fault tolerance than a 1+0. If
          multiple disks fail on one side of the mirror, then the other
          mirror is available. However, if disks are lost concurrently on
          both sides of the mirror, all data is lost.
         </para>
         <para>
          This solution offers less disk fault tolerance than a 1+0
          solution, but if you need to perform maintenance or maintain the
          mirror on a different site, you can take an entire side of the
          mirror offline and still have a fully functional storage device.
          Also, if you lose the connection between the two sites, either
          site operates independently of the other. That is not true if you
          stripe the mirrored segments, because the mirrors are managed at a
          lower level.
         </para>
         <para>
          If a device fails, the mirror on that side fails because RAID 1 is
          not fault-tolerant. Create a new RAID 0 to replace the failed
          side, then resynchronize the mirrors.
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   </sect2>

   <sect2 id="raidmdadmr10nest1">
    <title>Creating Nested RAID 10 (1+0) with mdadm</title>
    <para>
     A nested RAID 1+0 is built by creating two or more RAID 1 (mirror)
     devices, then using them as component devices in a RAID 0.
    </para>
    <important>
     <para>
      If you need to manage multiple connections to the devices, you must
      configure multipath I/O before configuring the RAID devices. For
      information, see
      <xref linkend="multipathing" xrefstyle="ChapTitleOnPage"/>.
     </para>
    </important>
    <para>
     The procedure in this section uses the device names shown in the
     following table. Ensure that you modify the device names with the names
     of your own devices.
    </para>
    <table id="b6670cx" frame="topbot" rowsep="0" pgwide="0">
     <title>Scenario for Creating a RAID 10 (1+0) by Nesting</title>
     <tgroup cols="3">
      <colspec colnum="1" colname="1" colwidth="3334*"/>
      <colspec colnum="2" colname="2" colwidth="3334*"/>
      <colspec colnum="3" colname="3" colwidth="3334*"/>
      <thead>
       <row id="b6670cy">
        <entry>
         <para>
          Raw Devices
         </para>
        </entry>
        <entry>
         <para>
          RAID 1 (mirror)
         </para>
        </entry>
        <entry>
         <para>
          RAID 1+0 (striped mirrors)
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b8o7og9">
        <entry>
         <simplelist>
          <member><filename>/dev/sdb1</filename>
          </member>
          <member><filename>/dev/sdc1</filename>
          </member>
         </simplelist>
        </entry>
        <entry>
         <para>
          <filename>/dev/md0</filename>
         </para>
        </entry>
        <entry morerows="1">
         <para/>
         <para>
          <filename>/dev/md2</filename>
         </para>
        </entry>
       </row>
       <row id="b8o7ogb">
        <entry>
         <simplelist>
          <member><filename>/dev/sdd1</filename>
          </member>
          <member><filename>/dev/sde1</filename>
          </member>
         </simplelist>
        </entry>
        <entry>
         <para>
          <filename>/dev/md1</filename>
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <procedure id="b6670d3">
     <step id="b6670d4">
      <para>
       Open a terminal console, then log in as the
       <systemitem>root</systemitem> user or equivalent.
      </para>
     </step>
     <step id="b6670d5">
      <para>
       Create 2 software RAID 1 devices, using two different devices for
       each RAID 1 device. At the command prompt, enter these two commands:
      </para>
<screen>
mdadm --create /dev/md0 --run --level=1 --raid-devices=2 /dev/sdb1 /dev/sdc1
</screen>
<screen>
mdadm --create /dev/md1 --run --level=1 --raid-devices=2 /dev/sdd1 /dev/sde1
</screen>
     </step>
     <step id="b6670d6">
      <para>
       Create the nested RAID 1+0 device. At the command prompt, enter the
       following command using the software RAID 1 devices you created in
       <xref linkend="b6670d5" xrefstyle="StepXRef"/>:
      </para>
<screen>
mdadm --create /dev/md2 --run --level=0 --chunk=64 --raid-devices=2 /dev/md0 /dev/md1
</screen>
      <para>
       The default chunk size is 64 KB.
      </para>
     </step>
     <step id="b6670d7">
      <para>
       Create a file system on the RAID 1+0 device
       <filename>/dev/md2</filename>, such as a Reiser file system
       (reiserfs). For example, at the command prompt, enter
      </para>
<screen>
mkfs.reiserfs /dev/md2
</screen>
      <para>
       Modify the command if you want to use a different file system.
      </para>
     </step>
     <step id="b6670d8">
      <para>
       Edit the <filename>/etc/mdadm.conf</filename> file to add entries for
       the component devices and the RAID device
       <filename>/dev/md2</filename>.
      </para>
     </step>
     <step id="b6670d9">
      <para>
       Edit the <filename>/etc/fstab</filename> file to add an entry for the
       RAID 1+0 device <filename>/dev/md2</filename>.
      </para>
     </step>
     <step id="b6670da">
      <para>
       Reboot the server.
      </para>
      <para>
       The RAID 1+0 device is mounted to <filename>/local</filename>.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 id="raidmdadmr10nest2">
    <title>Creating Nested RAID 10 (0+1) with mdadm</title>
    <para>
     A nested RAID 0+1 is built by creating two to four RAID 0 (striping)
     devices, then mirroring them as component devices in a RAID 1.
    </para>
    <important>
     <para>
      If you need to manage multiple connections to the devices, you must
      configure multipath I/O before configuring the RAID devices. For
      information, see
      <xref linkend="multipathing" xrefstyle="ChapTitleOnPage"/>.
     </para>
    </important>
    <para>
     In this configuration, spare devices cannot be specified for the
     underlying RAID 0 devices because RAID 0 cannot tolerate a device loss.
     If a device fails on one side of the mirror, you must create a
     replacement RAID 0 device, than add it into the mirror.
    </para>
    <para>
     The procedure in this section uses the device names shown in the
     following table. Ensure that you modify the device names with the names
     of your own devices.
    </para>
    <table id="b8gzg7s" frame="topbot" rowsep="0" pgwide="0">
     <title>Scenario for Creating a RAID 10 (0+1) by Nesting</title>
     <tgroup cols="3">
      <colspec colnum="1" colname="1" colwidth="3334*"/>
      <colspec colnum="2" colname="2" colwidth="3334*"/>
      <colspec colnum="3" colname="3" colwidth="3334*"/>
      <thead>
       <row id="b8gzg7t">
        <entry>
         <para>
          Raw Devices
         </para>
        </entry>
        <entry>
         <para>
          RAID 0 (stripe)
         </para>
        </entry>
        <entry>
         <para>
          RAID 0+1 (mirrored stripes)
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="bi71dhe">
        <entry>
         <simplelist>
          <member><filename>/dev/sdb1</filename>
          </member>
          <member><filename>/dev/sdc1</filename>
          </member>
         </simplelist>
        </entry>
        <entry>
         <para>
          <filename>/dev/md0</filename>
         </para>
        </entry>
        <entry morerows="1">
         <para/>
         <para>
          <filename>/dev/md2</filename>
         </para>
        </entry>
       </row>
       <row id="bi71dhf">
        <entry>
         <simplelist>
          <member><filename>/dev/sdd1</filename>
          </member>
          <member><filename>/dev/sde1</filename>
          </member>
         </simplelist>
        </entry>
        <entry>
         <para>
          <filename>/dev/md1</filename>
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <procedure id="b8gzg7y">
     <step id="b8gzg7z">
      <para>
       Open a terminal console, then log in as the root user or equivalent.
      </para>
     </step>
     <step id="b8gzg80">
      <para>
       Create two software RAID 0 devices, using two different devices for
       each RAID 0 device. At the command prompt, enter these two commands:
      </para>
<screen>
mdadm --create /dev/md0 --run --level=0 --chunk=64 --raid-devices=2 /dev/sdb1 /dev/sdc1
</screen>
<screen>
mdadm --create /dev/md1 --run --level=0 --chunk=64 --raid-devices=2 /dev/sdd1 /dev/sde1
</screen>
      <para>
       The default chunk size is 64 KB.
      </para>
     </step>
     <step id="b8gzg81">
      <para>
       Create the nested RAID 0+1 device. At the command prompt, enter the
       following command using the software RAID 0 devices you created in
       <xref linkend="b8gzg80" xrefstyle="StepXRef"/>:
      </para>
<screen>
mdadm --create /dev/md2 --run --level=1 --raid-devices=2 /dev/md0 /dev/md1
</screen>
     </step>
     <step id="b8gzg82">
      <para>
       Create a file system on the RAID 0+1 device
       <filename>/dev/md2</filename>, such as a Reiser file system
       (reiserfs). For example, at the command prompt, enter
      </para>
<screen>
mkfs.reiserfs /dev/md2
</screen>
      <para>
       Modify the command if you want to use a different file system.
      </para>
     </step>
     <step id="b8gzg83">
      <para>
       Edit the <filename>/etc/mdadm.conf</filename> file to add entries for
       the component devices and the RAID device
       <filename>/dev/md2</filename>.
      </para>
     </step>
     <step id="b8gzg84">
      <para>
       Edit the <filename>/etc/fstab</filename> file to add an entry for the
       RAID 0+1 device <filename>/dev/md2</filename>.
      </para>
     </step>
     <step id="b8gzg85">
      <para>
       Reboot the server.
      </para>
      <para>
       The RAID 0+1 device is mounted to <filename>/local</filename>.
      </para>
     </step>
    </procedure>
   </sect2>
  </sect1>
  <sect1 id="raidmdadmr10cpx">
   <title>Creating a Complex RAID 10</title>

   <itemizedlist role="subtoc">
    <listitem>
     <para>
      <xref linkend="raidmdadmr10cpxovw" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="raidmdadmr10cpxcreate" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="b14drcbo" xrefstyle="SectTitleOnPage"/>
     </para>
    </listitem>
   </itemizedlist>

   <sect2 id="raidmdadmr10cpxovw">
    <title>Understanding the Complex RAID10</title>
    <para>
     In <command>mdadm</command>, the RAID10 level creates a single complex
     software RAID that combines features of both RAID 0 (striping) and RAID
     1 (mirroring). Multiple copies of all data blocks are arranged on
     multiple drives following a striping discipline. Component devices
     should be the same size.
    </para>
    <itemizedlist role="subtoc">
     <listitem>
      <para>
       <xref linkend="b8ghyxy" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b7cym9j" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b7cym15" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b7cyneb" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="b7cynnk" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="byz81ho" xrefstyle="HeadingOnPage"/>
      </para>
     </listitem>
    </itemizedlist>
    <sect3 id="b8ghyxy">
     <title>Comparing the Complex RAID10 and Nested RAID 10 (1+0)</title>
     <para role="intro">
      The complex RAID 10 is similar in purpose to a nested RAID 10 (1+0),
      but differs in the following ways:
     </para>
     <table id="b8ghz5p" frame="topbot" rowsep="1" pgwide="0">
      <title>Complex vs. Nested RAID 10</title>
      <tgroup cols="3">
       <colspec colnum="1" colname="1" colwidth="3334*"/>
       <colspec colnum="2" colname="2" colwidth="3334*"/>
       <colspec colnum="3" colname="3" colwidth="3334*"/>
       <thead>
        <row id="b8gzgca">
         <entry>
          <para>
           Feature
          </para>
         </entry>
         <entry>
          <para>
           Complex RAID10
          </para>
         </entry>
         <entry>
          <para>
           Nested RAID 10 (1+0)
          </para>
         </entry>
        </row>
       </thead>
       <tbody>
        <row id="b8gzgcb">
         <entry>
          <para>
           Number of devices
          </para>
         </entry>
         <entry>
          <para>
           Allows an even or odd number of component devices
          </para>
         </entry>
         <entry>
          <para>
           Requires an even number of component devices
          </para>
         </entry>
        </row>
        <row id="b8gzgcc">
         <entry>
          <para>
           Component devices
          </para>
         </entry>
         <entry>
          <para>
           Managed as a single RAID device
          </para>
         </entry>
         <entry>
          <para>
           Manage as a nested RAID device
          </para>
         </entry>
        </row>
        <row id="b8gzgcd">
         <entry>
          <para>
           Striping
          </para>
         </entry>
         <entry>
          <para>
           Striping occurs in the near or far layout on component devices.
          </para>
          <para>
           The far layout provides sequential read throughput that scales by
           number of drives, rather than number of RAID 1 pairs.
          </para>
         </entry>
         <entry>
          <para>
           Striping occurs consecutively across component devices
          </para>
         </entry>
        </row>
        <row id="b8gzgce">
         <entry>
          <para>
           Multiple copies of data
          </para>
         </entry>
         <entry>
          <para>
           Two or more copies, up to the number of devices in the array
          </para>
         </entry>
         <entry>
          <para>
           Copies on each mirrored segment
          </para>
         </entry>
        </row>
        <row id="b8gzgcf">
         <entry>
          <para>
           Hot spare devices
          </para>
         </entry>
         <entry>
          <para>
           A single spare can service all component devices
          </para>
         </entry>
         <entry>
          <para>
           Configure a spare for each underlying mirrored array, or
           configure a spare to serve a spare group that serves all mirrors.
          </para>
         </entry>
        </row>
       </tbody>
      </tgroup>
     </table>
    </sect3>
    <sect3 id="b7cym9j">
     <title>Number of Replicas in the Complex RAID10</title>
     <para>
      When configuring an complex RAID10 array, you must specify the number
      of replicas of each data block that are required. The default number
      of replicas is 2, but the value can be 2 to the number of devices in
      the array.
     </para>
    </sect3>
    <sect3 id="b7cym15">
     <title>Number of Devices in the Complex RAID10</title>
     <para>
      You must use at least as many component devices as the number of
      replicas you specify. However, number of component devices in a RAID10
      array does not need to be a multiple of the number of replicas of each
      data block. The effective storage size is the number of devices
      divided by the number of replicas.
     </para>
     <para>
      For example, if you specify 2 replicas for an array created with 5
      component devices, a copy of each block is stored on two different
      devices. The effective storage size for one copy of all data is 5/2 or
      2.5 times the size of a component device.
     </para>
    </sect3>
    <sect3 id="b7cyneb">
     <title>Near Layout</title>
     <para>
      With the near layout, copies of a block of data are striped near each
      other on different component devices. That is, multiple copies of one
      data block are at similar offsets in different devices. Near is the
      default layout for RAID10. For example, if you use an odd number of
      component devices and two copies of data, some copies are perhaps one
      chunk further into the device.
     </para>
     <para>
      The near layout for the <command>mdadm</command> RAID10 yields read
      and write performance similar to RAID 0 over half the number of
      drives.
     </para>
     <para role="intro">
      Near layout with an even number of disks and two replicas:
     </para>
<screen>
sda1 sdb1 sdc1 sde1
  0&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;1
  2&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;3
  4&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;5
  6&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;7&nbsp;&nbsp;&nbsp;&nbsp;7
  8&nbsp;&nbsp;&nbsp;&nbsp;8&nbsp;&nbsp;&nbsp;&nbsp;9&nbsp;&nbsp;&nbsp;&nbsp;9
</screen>
     <para>
      Near layout with an odd number of disks and two replicas:
     </para>
<screen>
sda1 sdb1 sdc1 sde1 sdf1
  0&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;2
  2&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;4
  5&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;7
  7&nbsp;&nbsp;&nbsp;&nbsp;8&nbsp;&nbsp;&nbsp;&nbsp;8&nbsp;&nbsp;&nbsp;&nbsp;9&nbsp;&nbsp;&nbsp;&nbsp;9
  10   10   11&nbsp;&nbsp;&nbsp;11&nbsp;&nbsp;&nbsp;12
</screen>
    </sect3>
    <sect3 id="b7cynnk">
     <title>Far Layout</title>
     <para>
      The far layout stripes data over the early part of all drives, then
      stripes a second copy of the data over the later part of all drives,
      making sure that all copies of a block are on different drives. The
      second set of values starts halfway through the component drives.
     </para>
     <para>
      With a far layout, the read performance of the
      <command>mdadm</command> RAID10 is similar to a RAID 0 over the full
      number of drives, but write performance is substantially slower than a
      RAID 0 because there is more seeking of the drive heads. It is best
      used for read-intensive operations such as for read-only file servers.
     </para>
     <para>
      The speed of the raid10 for writing is similar to other mirrored RAID
      types, like raid1 and raid10 using near layout, as the elevator of the
      file system schedules the writes in a more optimal way than raw
      writing. Using raid10 in the far layout well-suited for mirrored
      writing applications.
     </para>
     <para role="intro">
      Far layout with an even number of disks and two replicas:
     </para>
<screen>
sda1 sdb1 sdc1 sde1
  0&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;3
  4&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;7       
  . . .
  3&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;2
  7&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;6
</screen>
     <para role="intro">
      Far layout with an odd number of disks and two replicas:
     </para>
<screen>
sda1 sdb1 sdc1 sde1 sdf1
  0&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;4
  5&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;7&nbsp;&nbsp;&nbsp;&nbsp;8&nbsp;&nbsp;&nbsp;&nbsp;9
  . . .
  4&nbsp;&nbsp;&nbsp;&nbsp;0&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;3
  9&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;7&nbsp;&nbsp;&nbsp;&nbsp;8
</screen>
    </sect3>
    <sect3 id="byz81ho">
     <title>Offset Layout</title>
     <para>
      The offset layout duplicates stripes so that the multiple copies of a
      given chunk are laid out on consecutive drives and at consecutive
      offsets. Effectively, each stripe is duplicated and the copies are
      offset by one device. This should give similar read characteristics to
      a far layout if a suitably large chunk size is used, but without as
      much seeking for writes.
     </para>
     <para role="intro">
      Offset layout with an even number of disks and two replicas:
     </para>
<screen>
sda1 sdb1 sdc1 sde1
  0&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;3
  3&nbsp;&nbsp;&nbsp; 0&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp;&nbsp;&nbsp;2       
  4    5    6    7
  7&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;6
  8    9   10   11
 11    8    9   10
</screen>
     <para role="intro">
      Offset layout with an odd number of disks and two replicas:
     </para>
<screen>
sda1 sdb1 sdc1 sde1 sdf1
  0&nbsp;&nbsp;&nbsp;&nbsp;1&nbsp;&nbsp;&nbsp;&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;4
  4    0    1    2    3
  5    6    7    8    9
  9    5    6    7    8
 10   11   12   13   14
 14   10   11   12   13&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</screen>
    </sect3>
   </sect2>

   <sect2 id="raidmdadmr10cpxcreate">
    <title>Creating a Complex RAID 10 with mdadm</title>
    <para>
     The RAID10 option for <command>mdadm</command> creates a RAID 10 device
     without nesting. For information about RAID10, see
     <xref linkend="raidmdadmr10cpxovw" xrefstyle="SectTitleOnPage"/>.
    </para>
    <para>
     The procedure in this section uses the device names shown in the
     following table. Ensure that you modify the device names with the names
     of your own devices.
    </para>
    <table id="b667706" frame="topbot" rowsep="1" pgwide="0">
     <title>Scenario for Creating a RAID 10 Using the mdadm RAID10 Option</title>
     <tgroup cols="2">
      <colspec colnum="1" colname="1" colwidth="5001*"/>
      <colspec colnum="2" colname="2" colwidth="5001*"/>
      <thead>
       <row id="b667707">
        <entry>
         <para>
          Raw Devices
         </para>
        </entry>
        <entry>
         <para>
          RAID10 (near or far striping scheme)
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row id="b667708">
        <entry>
         <para>
          <filename>/dev/sdf1</filename>
         </para>
         <para>
          <filename>/dev/sdg1</filename>
         </para>
         <para>
          <filename>/dev/sdh1</filename>
         </para>
         <para>
          <filename>/dev/sdi1</filename>
         </para>
        </entry>
        <entry>
         <para>
          <filename>/dev/md3</filename>
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <procedure id="b66770c">
     <step id="b667c10">
      <para>
       In YaST, create a 0xFD Linux RAID partition on the devices you want
       to use in the RAID, such as <filename>/dev/sdf1</filename>,
       <filename>/dev/sdg1</filename>, <filename>/dev/sdh1</filename>, and
       <filename>/dev/sdi1</filename>.
      </para>
     </step>
     <step id="b66770d">
      <para>
       Open a terminal console, then log in as the root user or equivalent.
      </para>
     </step>
     <step id="b66770e">
      <para>
       Create a RAID 10 command. At the command prompt, enter (all on the
       same line):
      </para>
<screen>
mdadm --create /dev/md3 --run --level=10 --chunk=4 --raid-devices=4 /dev/sdf1 /dev/sdg1 /dev/sdh1 /dev/sdi1
</screen>
     </step>
     <step id="b66770g">
      <para>
       Create a Reiser file system on the RAID 10 device
       <filename>/dev/md3</filename>. At the command prompt, enter
      </para>
<screen>
mkfs.reiserfs /dev/md3
</screen>
     </step>
     <step id="b66770h">
      <para>
       Edit the <filename>/etc/mdadm.conf</filename> file to add entries for
       the component devices and the RAID device
       <filename>/dev/md3</filename>. For example:
      </para>
<screen>
DEVICE /dev/md3
</screen>
     </step>
     <step id="b66770i">
      <para>
       Edit the <filename>/etc/fstab</filename> file to add an entry for the
       RAID 10 device <filename>/dev/md3</filename>.
      </para>
     </step>
     <step id="b66770j">
      <para>
       Reboot the server.
      </para>
      <para>
       The RAID10 device is mounted to <filename>/raid10</filename>.
      </para>
     </step>
    </procedure>
   </sect2>

   <sect2 id="b14drcbo">
    <title>Creating a Complex RAID10 with the YaST Partitioner</title>
    <procedure id="b14dtk6s">
     <step id="b14dtk6t">
      <para>
       Launch YaST as the <systemitem>root</systemitem> user, then open the
       Partitioner.
      </para>
     </step>
     <step id="b14dtk6u">
      <para>
       Select <guimenu>Hard Disks</guimenu> to view the available disks,
       such as sdab, sdc, sdd, and sde.
      </para>
     </step>
     <step id="b14dtk6v">
      <para>
       For each disk that you will use in the software RAID, create a RAID
       partition on the device. Each partition should be the same size. For
       a RAID 10 device, you need
      </para>
      <substeps>
       <step id="b14dtk6w">
        <para>
         Under <guimenu>Hard Disks</guimenu>, select the device, then select
         the <guimenu>Partitions</guimenu> tab in the right panel.
        </para>
       </step>
       <step id="b14dtk6x">
        <para>
         Click <guimenu>Add</guimenu> to open the <guimenu>Add
         Partition</guimenu> wizard.
        </para>
       </step>
       <step id="b14dtk6y">
        <para>
         Under <guimenu>New Partition Type</guimenu>, select
         <guimenu>Primary Partition</guimenu>, then click
         <guimenu>Next</guimenu>.
        </para>
       </step>
       <step id="b14dtk6z">
        <para>
         For <guimenu>New Partition Size</guimenu>, specify the desired size
         of the RAID partition on this disk, then click
         <guimenu>Next</guimenu>.
        </para>
       </step>
       <step id="b14dtk70">
        <para>
         Under <guimenu>Formatting Options</guimenu>, select <guimenu>Do not
         format partition</guimenu>, then select <guimenu>0xFD Linux
         RAID</guimenu> from the <guimenu>File system ID</guimenu> drop-down
         list.
        </para>
       </step>
       <step id="b14dtk71">
        <para>
         Under <guimenu>Mounting Options</guimenu>, select <guimenu>Do not
         mount partition</guimenu>, then click <guimenu>Finish</guimenu>.
        </para>
       </step>
       <step id="b14dtk72">
        <para>
         Repeat these steps until you have defined a RAID partition on the
         disks you want to use in the RAID 10 device.
        </para>
       </step>
      </substeps>
     </step>
     <step id="b14dtk73">
      <para>
       Create a RAID 10 device:
      </para>
      <substeps>
       <step id="b14dtk74">
        <para>
         Select <guimenu>RAID</guimenu>, then select <guimenu>Add
         RAID</guimenu> in the right panel to open the <guimenu>Add
         RAID</guimenu> wizard.
        </para>
       </step>
       <step id="b14dtk75">
        <para>
         Under <guimenu>RAID Type</guimenu>, select <guimenu>RAID 10
         (Mirroring and Striping)</guimenu>.
        </para>
        <informalfigure pgwide="0">
         <mediaobject>
          <imageobject role="fo">
           <imagedata fileref="raid10_a.png" width="302pt" format="PNG"/>
          </imageobject>
          <imageobject role="html">
           <imagedata fileref="raid10_a.png" width="302pt" format="PNG"/>
          </imageobject>
         </mediaobject>
        </informalfigure>
       </step>
       <step id="b14dtk76">
        <para>
         In the <guimenu>Available Devices</guimenu> list, select the
         desired Linux RAID partitions, then click <guimenu>Add</guimenu> to
         move them to the <guimenu>Selected Devices</guimenu> list.
        </para>
       </step>
       <step id="b14dtk77">
        <para>
         (Optional) Click <guimenu>Classify</guimenu>, specify the preferred
         order of the disks in the RAID array.
        </para>
        <para>
         For RAID types where the order of added disks matters, you can
         specify the order in which the devices will be used to ensure that
         one half of the array resides on one disk subsystem and the other
         half of the array resides on a different disk subsystem. For
         example, if one disk subsystem fails, the system keeps running from
         the second disk subsystem.
        </para>
        <substeps>
         <step id="b14dutul">
          <para>
           Select each disk in turn and click one of the <guimenu>Class
           X</guimenu> buttons, where X is the letter you want to assign to
           the disk. Available classes are A, B, C, D and E but for many
           cases fewer classes are needed (e.g. only A and B). Assign all
           available RAID disks this way.
          </para>
          <para>
           You can press the Ctrl or Shift key to select multiple devices.
           You can also right-click a selected device and choose the
           appropriate class from the context menu.
          </para>
         </step>
         <step id="b14dutum">
          <para>
           Specify the order the devices by selecting one of the sorting
           options:
          </para>
          <formalpara id="b14dutun">
           <title>Sorted:</title>
           <para>
            Sorts all devices of class A before all devices of class B and
            so on. For example: <literal>AABBCC</literal>.
           </para>
          </formalpara>
          <formalpara id="b14dutuo">
           <title>Interleaved:</title>
           <para>
            Sorts devices by the first device of class A, then first device
            of class B, then all the following classes with assigned
            devices. Then the second device of class A, the second device of
            class B, and so on follows. All devices without a class are
            sorted to the end of devices list. For example,
            <literal>ABCABC</literal>.
           </para>
          </formalpara>
          <formalpara id="b14dutup">
           <title>Pattern File:</title>
           <para>
            Select an existing file that contains multiple lines, where each
            is a regular expression and a class name (<literal>"sda.*
            A"</literal>). All devices that match the regular expression are
            assigned to the specified class for that line. The regular
            expression is matched against the kernel name
            (<filename>/dev/sda1</filename>), the udev path name
            (<filename>/dev/disk/by-path/pci-0000:00:1f.2-scsi-0:0:0:0-part1</filename>)
            and then the udev ID
            (/dev/disk/by-id/ata-ST3500418AS_9VMN8X8L-part1). The first
            match made determines the class if a device’s name matches
            more then one regular expression.
           </para>
          </formalpara>
         </step>
         <step id="b14dutuq">
          <para>
           At the bottom of the dialog box, click <guimenu>OK</guimenu> to
           confirm the order.
          </para>
          <informalfigure pgwide="0">
           <mediaobject>
            <imageobject role="fo">
             <imagedata fileref="raid10_classify_a.png" width="292pt" format="PNG"/>
            </imageobject>
            <imageobject role="html">
             <imagedata fileref="raid10_classify_a.png" width="292pt" format="PNG"/>
            </imageobject>
           </mediaobject>
          </informalfigure>
         </step>
        </substeps>
       </step>
       <step id="b14dttix">
        <para>
         Click <guimenu>Next</guimenu>.
        </para>
       </step>
       <step id="b14dttiy">
        <para>
         Under <guimenu>RAID Options</guimenu>, specify the C<guimenu>hunk
         Size</guimenu> and <guimenu>Parity Algorithm</guimenu>, then click
         <guimenu>Next</guimenu>.
        </para>
        <para>
         For a RAID 10, the parity options are n (near), f (far), and o
         (offset). The number indicates the number of replicas of each data
         block are required. Two is the default. For information, see
         <xref linkend="raidmdadmr10cpxovw" xrefstyle="SectTitleOnPage"/>.
        </para>
       </step>
       <step id="b14dttiz">
        <para>
         Add a file system and mount options to the RAID device, then click
         <guimenu>Finish</guimenu>.
        </para>
       </step>
      </substeps>
     </step>
     <step id="b14dttj0">
      <para role="intro">
       Select <guimenu>RAID</guimenu>, select the newly created RAID device,
       then click <guimenu>Used Devices</guimenu> to view its partitions.
      </para>
      <informalfigure pgwide="0">
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="raid10_useddevs_a.png" width="302pt" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="raid10_useddevs_a.png" width="302pt" format="PNG"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step id="b14dtw8j">
      <para>
       Click <guimenu>Next</guimenu>.
      </para>
     </step>
     <step id="b14dtw8k">
      <para>
       Verify the changes to be made, then click <guimenu>Finish</guimenu>
       to create the RAID.
      </para>
     </step>
    </procedure>
   </sect2>
  </sect1>
  <sect1 id="raidmdadmdegraded">
   <title>Creating a Degraded RAID Array</title>

   <para role="intro">
    A degraded array is one in which some devices are missing. Degraded
    arrays are supported only for RAID 1, RAID 4, RAID 5, and RAID 6. These
    RAID types are designed to withstand some missing devices as part of
    their fault-tolerance features. Typically, degraded arrays occur when a
    device fails. It is possible to create a degraded array on purpose.
   </para>

   <informaltable frame="topbot" rowsep="1" pgwide="0">
    <tgroup cols="3">
     <colspec colnum="1" colname="1" colwidth="3334*"/>
     <colspec colnum="2" colname="2" colwidth="3334*"/>
     <colspec colnum="3" colname="3" colwidth="3334*"/>
     <thead>
      <row id="b8gzm1p">
       <entry>
        <para>
         RAID Type
        </para>
       </entry>
       <entry>
        <para>
         Allowable Number of Slots Missing
        </para>
       </entry>
       <entry/>
      </row>
     </thead>
     <tbody>
      <row id="b8gzm1q">
       <entry>
        <para>
         RAID 1
        </para>
       </entry>
       <entry>
        <para>
         All but one device
        </para>
       </entry>
       <entry/>
      </row>
      <row id="b8gzm1r">
       <entry>
        <para>
         RAID 4
        </para>
       </entry>
       <entry>
        <para>
         One slot
        </para>
       </entry>
       <entry/>
      </row>
      <row id="b8h0rww">
       <entry>
        <para>
         RAID 5
        </para>
       </entry>
       <entry>
        <para>
         One slot
        </para>
       </entry>
       <entry/>
      </row>
      <row id="b8gzm1s">
       <entry>
        <para>
         RAID 6
        </para>
       </entry>
       <entry>
        <para>
         One or two slots
        </para>
       </entry>
       <entry/>
      </row>
     </tbody>
    </tgroup>
   </informaltable>

   <para>
    To create a degraded array in which some devices are missing, simply
    give the word <literal>missing</literal> in place of a device name. This
    causes <command>mdadm</command> to leave the corresponding slot in the
    array empty.
   </para>

   <para>
    When creating a RAID 5 array, <command>mdadm</command> automatically
    creates a degraded array with an extra spare drive. This is because
    building the spare into a degraded array is generally faster than
    resynchronizing the parity on a non-degraded, but not clean, array. You
    can override this feature with the <literal>--force</literal> option.
   </para>

   <para>
    Creating a degraded array might be useful if you want create a RAID, but
    one of the devices you want to use already has data on it. In that case,
    you create a degraded array with other devices, copy data from the
    in-use device to the RAID that is running in degraded mode, add the
    device into the RAID, then wait while the RAID is rebuilt so that the
    data is now across all devices. An example of this process is given in
    the following procedure:
   </para>

   <procedure id="b8h2aqj">
    <step id="b8h2aqk">
     <para>
      Create a degraded RAID 1 device <filename>/dev/md0</filename>, using
      one single drive <filename>/dev/sd1</filename>, enter the following at
      the command prompt:
     </para>
<screen>
mdadm --create /dev/md0 -l 1 -n 2 /dev/sda1 missing
</screen>
     <para>
      The device should be the same size or larger than the device you plan
      to add to it.
     </para>
    </step>
    <step id="b8h2atq">
     <para>
      If the device you want to add to the mirror contains data that you
      want to move to the RAID array, copy it now to the RAID array while it
      is running in degraded mode.
     </para>
    </step>
    <step id="b8h2avw">
     <para>
      Add a device to the mirror. For example, to add
      <filename>/dev/sdb1</filename> to the RAID, enter the following at the
      command prompt:
     </para>
<screen>
mdadm /dev/md0 -a /dev/sdb1
</screen>
     <para>
      You can add only one device at a time. You must wait for the kernel to
      build the mirror and bring it fully online before you add another
      mirror.
     </para>
    </step>
    <step id="b8h2ayp">
     <para>
      Monitor the build progress by entering the following at the command
      prompt:
     </para>
<screen>
cat /proc/mdstat
</screen>
     <para>
      To see the rebuild progress while being refreshed every second, enter
     </para>
<screen>
watch -n 1 cat /proc/mdstat 
</screen>
    </step>
   </procedure>
  </sect1>
 </chapter>
