<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>

<article id="article.vt.best.practices" lang="en">
 <?suse-quickstart color="suse"?>
 <title>Virtualization Best Practices</title>
 <subtitle>&sle; &productname; &productnumber;</subtitle>
 <articleinfo>
  <productnumber>&productnumber;</productnumber><productname>&sle;
  &productname;</productname>
 </articleinfo>

<!--
we can write it based on on scenario?
Virtualization Capabilities:
Consolidation (hardware1+hardware2 -> hardware)
 Isolation
 Migration (hardware1 -> hardware2)
 Disaster recovery
 Dynamic load balancing
-->

<sect1 id="vt.best.intro">
 <title>Taking into consideration</title>
 <para>
 </para>
 
 <sect2 id="vt.best.intro.backup">
  <title>Backup</title>
  <para>
  backup before doing anything   
  </para>
 </sect2>
 
 <sect2 id="vt.best.intro.acpi">
  <title>Acpi test</title>
  <para>
   testing acpi shutdown/poweroff before production   
  </para>
 </sect2>

 <sect2 id="vt.best.intro.libvirt">
  <title>Libvirt Framework</title>
  <para>
   usage of libvirt framework
   qemu-system-arch not visible under libvirt
  </para>
 </sect2>
 
 <sect2 id="vt.best.intro.qemu">
  <title>qemu-system-i386 VS qemu-system-x86_64</title>
  <para>
   Just as a modern 64 bit x86 PC supports running a 32 bit OS as well as a 64
   bit OS, <command>qemu-system-x86_64</command> runs 32 bit OS's perfectly fine, and in fact
   usually provides better performance to 32 bit guests than <command>qemu-system-i386</command>,
   which provides a 32 bit guest environment only. Hence we recommend using
   <command>qemu-system-x86_64</command> over
   <command>qemu-system-i386</command> for all guest types. Where <command>qemu-system-i386</command> is known to perform better is in configurations which SUSE does not support.
  </para>
 </sect2>
</sect1>


<sect1 id="vt.best.guest">
 <title>Guest options</title>
 <para>
 </para>
 
 <sect2 id="vt.best.guest.allocation">
  <title>Over Allocation</title>
  <para>
   avoid over-allocation on Guest   
  </para>
 </sect2>
 

 <sect2 id="vt.best.guest.kbd">
  <title>Keyboard Layout</title>
  <para>
   Keyboard layout option   
  </para>
 </sect2>
 
 <sect2 id="vt.best.guest.clock">
  <title>Clock Setting</title>
  <para>
   clock setting (common source)
   kvm clock and ntp   
  </para>
 </sect2>
 
 <sect2 id="vt.guest.guest.bio">
  <title>Bio-based</title>
  <para>
   bio-based driver on slow devices
  </para>
 </sect2>
</sect1>

<sect1 id="vt.best.perf">
 <title>Performance</title>
 <para>
 </para>
 <sect2 id="vt.best.perf.general">
  <title>General</title>
  <para>
Minimize software on host (reduce resource, reduce security/increase aviability)
PV drivers should be used (virtio_blk, virtio_net, virtio_balloon)
virtio-net recomendation (performance benefit)
performance cirrus: 16bit color
KSM and Page sharing
Swapping
I/O Scheduler
NFS storage
  </para>
<sect3 id="vt.best.perf.general.mtype">
 <title>Updating guest machine type</title>
 <para>
  QEMU machine types define some details of the architecture which are
  particularly relevant in the context of migration and save/restore, where all
  the details of the virtual machine ABI need to be carefully accounted for. As
  changes or improvements to QEMU are made or certain types of fixes are done,
  new machine types are created which include those changes. Though the older
  machine types used for supported versions of QEMU are still valid to use, the
  user would do well to try to move to the latest machine type supported by the
  current release, so as to be able to take advantage of all the changes
  represented in that machine type. Changing the guest's machine type for a
  Linux guest will mostly be transparent, whereas for Windows* guests, it is
  probably a good idea to take a snapshot or backup of the guest in case Windows*
  has issues with the changes it detects and subsequently the user decides to
  revert to the original machine type the guest was created with.
 </para>
</sect3>
 </sect2>
 
 <sect2 id="vt.best.perf.cpu">
  <title>CPU parameter</title>
  <para>
Avoid CPU overcommit
Scheduler
vCPU model and features: Normalize to allow migration among heterogeneous hosts
vCPU topology: Multiple sockets with a single core and thread generally give best performance
vCPU Pinning : Constrain vCPU threads to a NUMA node
  </para>
 </sect2>
 <sect2 id="vt.best.perf.numa">
  <title>NUMA</title>
  <para>
Potentially huge impact on performance
Consider host topology when sizing guests
Avoid allocating VM memory across NUMA nodes
Prevent vCPUs from floating across NUMA nodes
  </para>
 </sect2>
 <sect2 id="vt.best.perf.mem">
  <title>Memory</title>
  <para>
Hugepages
HugeTLB
Policy for allocation in NUMA topology:  strict,  interleave,  preferred
  </para>
 </sect2>
</sect1>


<sect1 id="vt.best.stor">
 <title>Storage and Filesystem</title>
 <para>
  
 </para>

 <sect2 id="vt.best.stor.general">
  <title>General</title>
  <para>
   Access CD/DVD -> storage pool
deleting pool
Brtfs and guest image
SCSI Host Adapter name (avoid /dev/sdX)
qemu direct access to host drives (-drive file=)
QEMU Storage formats
  </para>
 </sect2>
 <sect2 id="vt.best.stor.blkvsimg">
  <title>BLK VS Images files</title>
  <para>
Block devices
 Better performance
 Use “standard” tools for administration/disk modification
 Accessible from host (pro and con)

Image Files
 Easier system management
  Easily move, clone, backup domains
 Comprehensive toolkit (guestfs) for image manipulation
 Reduce overhead through sparse files
Fully allocate for best performance

  </para>
 </sect2>
 
 <sect2 id="vt.best.stor.imageformat">
  <title>Image Format</title>
  <para>
   Certain storage formats which QEMU recognizes have their origins in other
   virtualization technologies. By recognizing these formats, QEMU is able to
   leverage either data stores or entire guests which were originally targeted
   to run under these other virtualization technologies. Some of these formats
   are supported only in read-only mode, enabling either direct use of that read
   only data store in a QEMU guest or conversion to a fully supported QEMU
   storage format (using <command>qemu-img</command>) which could then be used in read/write mode.
   See &sle; <ulink url="https://www.suse.com/releasenotes/x86_64/SUSE-SLES/12/#fate-317891">Release Notes</ulink> to get the list of supported format.
  </para>
  <para>
raw
 Most common format
 Historically, best performance
qcow2
 Required for snapshot support in libvirt + tools
 Improved performance and stability
qed
 Next generation qcow

vhd/vhdx
 Also known as 'vpc'
vmdk
 Performance
 Suggest converting to raw or qcow2
<command>qemu-img convert -f vmdk -O qcow2 img.vmdk img.qcow2</command>
  </para>

 </sect2>

 <sect2 id="vt.best.stor.diskio">
  <title>Disk IO Modes</title>
  <para>
Disk IO Modes
native
 kernel asynchronous IO
threads
 host user-mode based threads
default
 'threads' mode in SLES
  </para>
 </sect2>
</sect1>


<sect1 id="vt.best.snapsav">
 <title>Saving, migrating and Snapshoting</title>
 <para>
Migration requirements/Recomendations
save VM and start/boot (memory invalid)
snapshot naming importance
avoid qemu-img snapshot
cache mode in live migration
guestfs and live system
 </para>
</sect1>


<sect1 id="vt.best.security">
 <title>Security consideration</title>
 <para>
Connection to guest: security policy
Authentication for libvirtd and VNC need to be configured separately
RNG and entropy
QEMU Guest Agent
The VNC TLS (set at start)
 </para>
</sect1>


<sect1 id="vt.best.pcipass">
 <title>pcpipass</title>
 <para>
Pci device (not online!, managed/unmanaged)
howto check SR-IOV capabilities
 </para>
</sect1>

<sect1 id="vt.best.net">
 <title>Network Tips</title>
 <para>
 </para>

 <sect2 id="vt.best.net.vnic">
  <title>Virtual NICs</title>
  <para>
 virtio-net (KVM) : multi-queue option
 vhost-net (KVM) : Default vNIC, best performance
 netbk (Xen) : kernel threads vs tasklets
  </para>
 </sect2>
 
 <sect2 id="vt.best.net.enic">
  <title>Emulated NICs</title>
  <para>
   e1000: Default and preferred emulated NIC
   rtl8139
  </para>
 </sect2>
 
 <sect2 id="vt.best.net.sharednic">
  <title>Shared Physical NICs</title>
  <para>
   SR-IOV: macvtap
   Physicial NICs : PCI passthrough
  </para>
 </sect2>

 <sect2 id="vt.best.general">
  <title>Network General</title>
  <para>
use multi-network to avoid congestion
 admin, storage, migration ...
 use arp-filter to prevent arp flux

same MTU in all devices to avoid fragmentation
yast to configure bridge
Network MAC address
bridge configuration in bridge.conf file
PCI passthrough Vfio to improve performance
  </para>
 </sect2>
</sect1>

<sect1 id="vt.best.debug">
 <title>Troubleshooting/Debuging</title>
 <para>
 </para>

 <sect2 id="vt.best.debug.xen">
  <title>Xen</title>
  <para>
  </para>
  <sect3 id="vt.best.debug.xen.log">
   <title>Xen Log Files</title>
   <para>
    libxl logs:
    /var/log/xen/*
     qemu-dm-domain.log, xl-domain.log
     bootloader.log, vm-install, xen-hotplug
       Process specific logs, often requiring debug log levels to be useful
       Some logs require 'set -x' to be added to /etc/xen/scripts/*

   libvirt logs:
   /var/log/libvirt/libxl
   libxl-driver.log
   domain.log
  </para>
  </sect3>
  <sect3 id="vt.best.debug.xen.hypervisor">
   <title>Daemon and Hypervisor Logs</title>
   <para>
    View systemd journal for specific units/daemons:
    <command>journalctl [--follow] –unit xencommons.service</command>
    <command>journalctl /usr/sbin/xenwatchdogd</command>
    xl dmesg
    Xen hypervisor logs
   </para>
  </sect3>
  
  <sect3 id="vt.best.debug.xen.loglevel">
   <title>Increasing Logging Levels</title>
   <para>
    Log levels are increased through xen parameters:
loglvl=all
 Increased logging for Xen hypervisor
guest_loglvl=all
 Increased logging for guest domain actions
Grub2 config:
 Edit <filename>/etc/default/grub</filename>, then recreate <filename>grub.cfg</filename>:
 GRUB_CMDLINE_XEN_DEFAULT=”loglvl=all guest_loglvl=all”
 <command>grub2-mkconfig -o /boot/grub/grub.cfg</command>
   </para>
  </sect3>
 </sect2>

 <sect2 id="vt.best.debug.support">
  <title>Support</title>
  <para></para>
  <sect3 id="vt.best.debug.support.config">
   <title>Supportconfig and Virtualization</title>
   <para>
    Core files:
    basic-environment.txt
    Reports detected virtualization hypervisor
    Under some hypervisors (xen), subsequent general checks might be incomplete
    
    Hypervisor specific files:
    kvm.txt, xen.txt
    Both logs contain general information:
    RPM version/verification of important packages
    Kernel, hardware, network details
   </para>
  </sect3>
  
  <sect3 id="vt.best.debug.support.kvm">
   <title>kvm.txt</title>
   <para>
    libvirt details
    General libvirt details
     Libvirt daemon status
     KVM statistics
     virsh version, capabilities, nodeinfo, etc...
     
    Domain list and configurations
    Conf and log files
     /etc/libvirt/libvirtd.conf
      Last 500 lines from /var/log/libvirt/qemu/domain.log
   </para>
  </sect3>

  <sect3 id="vt.best.debug.support.xen">
   <title>xen.txt</title>
   <para>
    Daemon status
     xencommons, xendomains and xen-watchdog daemons
      grub/grub2 configuration (for xen.gz parameters)

     libvirt details
      Domain list and configurations
     
     xl details
      Domain list and configurations
      Conf and Log files
       /etc/xen/xl.conf, /etc/libvirt/libvirtd.conf
       Last 500 lines from /var/log/xen/*, /var/log/libvirt/libxl/*
       Output of `xl dmesg` and `xl info`
   </para>
  </sect3>
 </sect2> 


 <sect2 id="vt.best.debug.advanced">
  <title>Advanced Debugging Options</title>
  <para>
   Serial console
    GRUB_CMDLINE_XEN_DEFAULT=“loglvl=all guest_loglvl=all console=com1 com1=115200,8n1”
    GRUB_CMDLINE_LINUX_DEFAULT=“console=ttyS0,115200”

    Debug keys
     xl debug keys h; xl dmesg
     xl debug keys q; xl dmesg   

     Additional Xen debug tools:
      xenstore-{ls,read,rm,watch,write}, xentrace, xentop
     
     Capturing Guest Logs
      Capturing guest logs during triggered problem:
       Connect to domain:
        virsh console domname
       Execute problem command
       
     Capturing domain boot messages
      xl create -c VM config file
      virsh create VM config file --console
  </para>
 </sect2>
 <sect2 id="vt.best.trouble">
  <title>Troubleshooting Installations</title>
  <para>
   virt-manager and virt-install logs:
    Found in ~/.cache/virt-manager

    Debugging virt-manager:
     virt-manager --no-fork
      Sends messages directly to screen and log file
     LIBVIRT_DEBUG=1 virt-manager --no-fork
      See libvirt messages in /var/log/messages

    Use 'xl' to rule out libvirt layer
  </para>
 </sect2>
 <sect2 id="vt.best.trouble.libvirt"> 
  <title>Troubleshooting Libvirt</title>
  <para>
   Client side troubleshooting
    LIBVIRT_DEBUG=1
    1: debug, 2: info, 3: warning, 4: error

   Server side troubleshooting
   <filename>/etc/libvirt/libvirtd.conf</filename> (restart libvirtd after changes)
   log_level = 1
   log_output = “1:file:/var/log/libvirtd.log”
  log_filters = “1:qemu 3:remote”
  </para>
 </sect2>

 <sect2 id="vt.best.trouble.kernel">
  <title>Kernel Cores</title>
  <para>
   Host cores -vs- guest domain cores
    Host cores are enabled through Kdump YaST module
     For Xen dom0 cores, 'crashkernel=size@offset' should be added as a Xen hypervisor parameter

    Guest cores require:
     on_crash[action]on_crash tag
      Possible coredump actions are:
       coredump-restart     Dump core, then restart the VM
       coredump-destroy    Dump core, then terminate the VM
     Crashes are written to:
      /var/lib/libvirt/{libxl,qemu}/dump
      /var/lib/xen/dump  (if using xl)
  </para>
 </sect2>
 
 <sect2 id="vt.best.debug.other">
  <title>Other</title>
  <para>
  VGA trouble debug
 </para>
 </sect2>
</sect1>
</article>
