<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<article xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="article.vt.best.practices" xml:lang="en">
<?suse-quickstart color="suse"?>
 <title>Virtualization Best Practices</title>
 <subtitle>&sle; &productname; &productnumber;</subtitle>
 <info>
  <productnumber>&productnumber;</productnumber><productname>&sle;
  &productname;</productname>
 </info>
<!--
we can write it based on on scenario?
Virtualization Capabilities:
Consolidation (hardware1+hardware2 -> hardware)
 Isolation
 Migration (hardware1 -> hardware2)
 Disaster recovery
 Dynamic load balancing
-->
 <sect1 xml:id="vt.best.scenario">
  <title>Virtualization Scenarios</title>

  <para>
   Virtualization offers a lot of capabilities to your environment. It can
   be used in multiple sort of scenario. To get more details about
   <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_virtualization_introduction_capabilities.html">Virtualization
   Capabilities</link> and
   <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_virtualization_introduction_benefits.html">Virtualization
   Benefits</link> please refer to the
   <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/book_virt.html">Virtualization
   Guide</link>.
  </para>

  <para>
   This best practice will provide you some advice to help you do the right
   choice in your environment, it will recommend or discourage the usage of
   options depending on your usage.
  </para>

<!-- TODO
 <table rowsep="1">
  <title>Scenario</title>
  <tgroup cols="3">
   <colspec colnum="1" colname="1" colwidth=""/>
   <colspec colnum="2" colname="2" colwidth=""/>
   <thead>
    <row>
     <entry>
      <para>Scenarios</para>
     </entry>
     <entry>
      <para>Option Recommended for</para>
     </entry>
     <entry>
      <para>Option Not recommended for</para>
     </entry>
    </row>
   </thead>
   <tbody>
    <row>
     <entry>
      <para>Consolidation</para>
     </entry>
     <entry><para>X</para></entry>
    </row>
    <row>
     <entry>
      <para>Isolation</para>
     </entry>
     <entry></entry>
     <entry><para>X</para></entry>
    </row>
    <row>
     <entry>
      <para>Migration</para>
     </entry>
     <entry><para>X</para></entry>
    </row>
    <row>
     <entry>
      <para>Disaster Recovery</para>
     </entry>
     <entry>
      <mediaobject>
       <imageobject>
        <imagedata fileref="thumbup_green.png" width="25%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </entry>
    </row>
    <row>
     <entry>
      <para>Dynamic Load Balancing</para>
     </entry>
    <entry></entry>
    <entry>
      <mediaobject>
       <imageobject>
        <imagedata fileref="thumbdown_red.png" width="25%" format="PNG"/>
       </imageobject>
      </mediaobject>
    </entry>
    </row>
   </tbody>
  </tgroup>
 </table>
-->
 </sect1>
 <sect1 xml:id="vt.best.intro">
  <title>Before Any Modification</title>

  <para>
   The efficiency of virtualization environment depends on administration
   choice. This guide is provided as a reference for doing good choice in
   production environment. Nothing is <emphasis>graved on stone</emphasis>,
   and different infra-structure can provide different result.
   <emphasis>Pre-experimentation</emphasis> is a key point to get a
   successfully virtualization environment.
  </para>

  <sect2 xml:id="vt.best.intro.backup">
   <title>Backup First</title>
   <para>
    Playing with &vmguest; and the Host configuration can lead to data loss
    or unstable state. It's really important that you do backups of files,
    data, images etc.. before doing any change. Without backups you won't be
    able to easily restore the original state after a data loss or a
    miss-configuration.
   </para>
   <warning>
    <para>
     Backup is mandatory before doing any tests to be sure you will be able
     to roll back to a usable/stable system or configuration. Don't do any
     test or experimentation on online production system.
    </para>
   </warning>
  </sect2>

  <sect2 xml:id="vt.best.intro.acpi">
   <title>Do ACPI Testing</title>
   <para>
    The capabilities to change a &vmguest; state heavily depends on the
    operating system. It's really important to test this features before any
    use of you &vmguest; in production. For example most of Linux OS disable
    this capabilities per default, so this require that you enable this
    operation (mostly through Policy Kit).
   </para>
   <para>
    ACPI must be enabled in the guest for a graceful shutdown to work. To
    check if ACPI is enabled, run:
   </para>
<screen>virsh dumpxml <replaceable>VMNAME</replaceable> | grep acpi</screen>
   <para>
    If nothing is printed, ACPI is not enabled for your machine. Use
    <command>virsh edit</command> to add the following XML under
    &lt;domain&gt;:
   </para>
<screen>&lt;features&gt;
  &lt;acpi/&gt;
&lt;/features&gt;</screen>
   <para>
    If ACPI was enabled during a Windows Server 20XX guest installation,
    turning it on in the &vmguest; configuration alone is not sufficient.
    See the following articles for more information:
   </para>
   <simplelist>
    <member><link xlink:href="http://support.microsoft.com/kb/314088/EN-US/"/>
    </member>
    <member><link xlink:href="http://support.microsoft.com/?kbid=309283"/>
    </member>
   </simplelist>
   <para>
    A graceful shutdown is of course always possible from within the guest
    operating system, regardless of the &vmguest;'s configuration.
   </para>
  </sect2>

  <sect2 xml:id="vt.best.intro.libvirt">
   <title>Prefer &libvirt; Framework</title>
   <para>
    In SUSE Linux Enterprise it is recommended to use the &libvirt;
    framework to do any operation on hosts, containers and &vmguest;. If you
    use other tools this may not appear or be propagated in you system. For
    example creating a system image by hand with <command>qemu-img</command>
    <option>create</option> data.raw 10G will not be displayed in the
    <command>virt-manager</command> pool section. If you use a
    <command>qemu-system-arch</command> command this will no be visible
    under &libvirt;. So you should carefully used any other management tools
    and keep in mind their usage won't be probably reflected on other tools.
   </para>
   <note>
    <para>
     Read
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/cha_libvirt_overview.html#">&libvirt;
     overview </link> for more information.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="vt.best.intro.qemu">
   <title>qemu-system-i386 VS qemu-system-x86_64</title>
   <para>
    Just as a modern 64 bit x86 PC supports running a 32 bit OS as well as a
    64 bit OS, <command>qemu-system-x86_64</command> runs 32 bit OS's
    perfectly fine, and in fact usually provides better performance to 32
    bit guests than <command>qemu-system-i386</command>, which provides a 32
    bit guest environment only. Hence we recommend using
    <command>qemu-system-x86_64</command> over
    <command>qemu-system-i386</command> for all guest types. Where
    <command>qemu-system-i386</command> is known to perform better is in
    configurations which SUSE does not support.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="vt.best.guest">
  <title>Guest Options</title>
  <para></para>
  <sect2 xml:id="vt.best.guest.kbd">
   <title>Keyboard Layout</title>
   <para>
    Even if it is possible to specify the keyboard layout from a
    <command>qemu-system-ARCH</command> command it's recommended to do this
    configuration in the &libvirt; XML file. To change the keyboard layout
    while connecting to a remote &vmguest; using vnc you should edit the
    &vmguest; XML configuration file. The XML is located at
    <filename>/etc/libvirt/<replaceable>HYPERVISOR</replaceable></filename>.
    For example to add an "en-us" keymap add in the &lt;devices&gt; section:
   </para>
<screen>&lt;graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/&gt;</screen>
   <para>
    Check the vncdisplay configuration and connect to your &vmguest;:
   </para>
<screen>virsh vncdisplay sles12 127.0.0.1:0</screen>
  </sect2>

<!-- TODO
 <sect2 id="vt.best.guest.clock">
  <title>Clock Setting</title>
  <para>
   
   clock setting (common source)
   kvm clock and ntp   
  </para>
 </sect2>
 
 <sect2 id="vt.guest.guest.bio">
  <title>Bio-based</title>
  <para>
   bio-based driver on slow devices
  </para>
 </sect2>
-->
 </sect1>
 <sect1 xml:id="vt.best.perf">
  <title>Performance</title>
  <para></para>
  <sect2 xml:id="vt.best.perf.virtio">
   <title>Virtio Driver</title>
   <para>
    To increase &vmguest; performance it's recommended to use PV drivers,
    the host implementation is in userspace, so no driver is needed in the
    host. virtio is a virtualization standard, so the guest's device driver
    is aware that it is running in a virtual environment. Note that virtio
    used in &kvm; is different, but architecturally similar to the &xen;
    paravirtualized device drivers (like
    <link xlink:href="https://www.suse.com/products/vmdriverpack/">VMDP</link> in
    a Windows guest).
   </para>
   <note>
    <title>I/O in Virtualization</title>
    <para>
     To have a better understanding on this topic please refer to
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_vt_io.html">I/O
     Virtualization</link> section in the official Virtualization guide.
    </para>
   </note>
   <sect3>
    <title>virtio blk</title>
    <para>
     <emphasis>virtio_blk</emphasis> is the virtio block device for disk.
    </para>
    <warning>
     <title>&qemu; option</title>
     <para>
      The <option>-hd[ab]</option> for virtio disk won't work, you must use
      <option>-drive</option> instead.
     </para>
    </warning>
    <warning>
     <title>Disk Name Change</title>
     <para>
      Disk will show up as <option>/dev/vd[a-z][1-9]</option>, if you
      migrate from a non-virtio disk you need to change
      <option>root=</option> in GRUB config, and regenerate the
      <filename>initrd</filename> file or the system won't be able to boot.
     </para>
    </warning>
    <para>
     Example of a virtio disk definition:
    </para>
<screen>&lt;disk type='....' device='disk'&gt;
    ....
    &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;</screen>
    <para>
     This is preferable to remove every disk block in the XML configuration
     containing <emphasis>&lt;address .*&gt;</emphasis> because it will be
     re-generated automatically.
    </para>
   </sect3>
   <sect3>
    <title>virtio net</title>
    <para>
     <emphasis>virtio_net</emphasis> is the virtio network device. The
     kernel modules should be insmoded automatically in the guest at boot
     time. You need to start the service to get network available.
    </para>
<screen>&lt;interface type='network'&gt;
    ...
    &lt;model type='virtio' /&gt;
&lt;/interface&gt;</screen>
   </sect3>
   <sect3>
    <title>virtio balloon</title>
    <para>
     <emphasis>virtio_balloon</emphasis> is a PV driver to give or take
     memory from a &vmguest;. It's controlled by
     <emphasis>currentMemory</emphasis> and <emphasis>memory</emphasis>
     option.
    </para>
<screen>&lt;memory unit='KiB'&gt;16777216&lt;/memory&gt;
  &lt;currentMemory unit='KiB'&gt;1048576&lt;/currentMemory&gt;
  [...]
  &lt;devices&gt;
    &lt;memballoon model='virtio'/&gt;
 &lt;/devices&gt;</screen>
   </sect3>
   <sect3>
    <title>Checking virtio Presence</title>
    <para>
     You can check the virtio block pci with:
    </para>
<screen># find /sys/devices/ -name virtio*
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2</screen>
    <para>
     To find the block device <filename>vdX</filename> associated:
    </para>
<screen># find /sys/devices/ -name virtio* -print  -exec ls {}/block 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
/sys/devices/pci0000:00/0000:00:07.0/virtio1
/sys/devices/pci0000:00/0000:00:08.0/virtio2
vda</screen>
    <para>
     Get more information on the virtio block:
    </para>
<screen># udevadm info -p /sys/devices/pci0000:00/0000:00:08.0/virtio2
P: /devices/pci0000:00/0000:00:08.0/virtio2
E: DEVPATH=/devices/pci0000:00/0000:00:08.0/virtio2
E: DRIVER=virtio_blk
E: MODALIAS=virtio:d00000002v00001AF4
E: SUBSYSTEM=virtio</screen>
    <para>
     Check all virtio driver used:
    </para>
<screen># find /sys/devices/ -name virtio* -print  -exec ls -l {}/driver 2&gt;/dev/null \;
/sys/devices/pci0000:00/0000:00:06.0/virtio0
lrwxrwxrwx 1 root root 0 Jun 17 15:48 /sys/devices/pci0000:00/0000:00:06.0/virtio0/driver -&gt; ../../../../bus/virtio/drivers/virtio_console
/sys/devices/pci0000:00/0000:00:07.0/virtio1
lrwxrwxrwx 1 root root 0 Jun 17 15:47 /sys/devices/pci0000:00/0000:00:07.0/virtio1/driver -&gt; ../../../../bus/virtio/drivers/virtio_balloon
/sys/devices/pci0000:00/0000:00:08.0/virtio2
lrwxrwxrwx 1 root root 0 Jun 17 14:35 /sys/devices/pci0000:00/0000:00:08.0/virtio2/driver -&gt; ../../../../bus/virtio/drivers/virtio_blk</screen>
   </sect3>
<!--
<para>
 Currently performance is much better when using a host kernel configured with <emphasis>CONFIG_HIGH_RES_TIMERS</emphasis>. Another option is use HPET/RTC and <option>-clock=</option> &qemu; option.
  </para>
  -->
  </sect2>

  <sect2 xml:id="vt.best.perf.cirrus">
   <title>Cirrus Video Driver</title>
   <para>
    To get 16bit color, high compatibility and better performance it's
    recommended to use the <emphasis>cirrus</emphasis> video driver.
   </para>
<screen>&lt;video&gt;
  &lt;model type='cirrus' vram='9216' heads='1'/&gt;
&lt;/video&gt;</screen>
  </sect2>

  <sect2 xml:id="vt.best.perf.ksm">
   <title>KSM and Page Sharing</title>
   <para>
    Kernel Share Memory is a kernel module to increase memory density by
    merging equal anonymous pages on a system. This free memory on the
    system allow to run more &vmguest; on the same host. Running same Guest
    on a host generate a lot of common memory on the host. You can enable
    KSM with <command>echo 1 &gt; /sys/kernel/mm/ksm/run</command>. One
    advantage of using KSM in a &vmguest; is that all guest memory is backed
    by host anonymous memory, so you can share
    <emphasis>pagecache</emphasis>, <emphasis>tmpfs</emphasis> or any kind
    of memory allocated on the guest.
   </para>
   <para>
    KSM is controlled by <emphasis>sysfs</emphasis>. You can check KSM's
    values in <filename>/sys/kernel/mm/ksm/</filename>:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <emphasis>pages_shared</emphasis>: how many shared pages with
      different content are being used (Read only).
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>pages_sharing</emphasis>: how many pages are being shared
      between your kvm guests (Read only).
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>pages_unshared</emphasis>: how many pages unique but
      repeatedly checked for merging (Read only).
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>pages_volatile</emphasis>: how many pages changing too fast
      to be placed in a tree (Read only).
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>full_scans</emphasis>: how many times all mergeable areas
      have been scanned (Read only).
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>sleep_millisecs</emphasis>: how many milliseconds
      <command>ksmd</command> should sleep before next scan. a low value
      will overuse the CPU, so you will loss CPU Time for other tasks,
      setting this to a value greater than <option>1000</option> is
      recommended.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>pages_to_scan</emphasis>: how many present pages to scan
      before ksmd goes to sleep. A big value will overuse the CPU, you can
      start with <option>1000</option>, and then adjust to an appropriate
      value based on KSM result.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>merge_across_nodes</emphasis>: by default the system merge
      page across NUMA node. set this option to <option>0</option> to
      disable this behavior.
     </para>
    </listitem>
   </itemizedlist>
   <note>
    <para>
     KSM should be used if you know that you will over-use your host system
     memory or you will run same instance of applications or &vmguest;. If
     this is not the case it's preferable to disable it. IE; in the XML
     configuration of the &vmguest; add:
    </para>
<screen>&lt;memoryBacking&gt;
   &lt;nosharepages/&gt;
&lt;/memoryBacking&gt;</screen>
   </note>
   <warning>
    <para>
     KSM can free up some memory on the host system, but the administrator
     should also reserve enough swap to avoid system running out of memory
     in case of the amount of share memory decrease (decrease of share
     memory will result on an increasement use of the physical memory).
    </para>
   </warning>
  </sect2>

  <sect2 xml:id="vt.best.perf.swap">
   <title>Swapping</title>
   <para>
    <emphasis>Swap</emphasis> is mostly used by the system to store
    under-used physical memory (low usage, or not accessed since a long
    time). To prevent the system running out of memory setting up a mimimum
    swap is highly recommended.
   </para>
   <sect3>
    <title>swappiness</title>
    <para>
     The <emphasis>swappiness</emphasis> setting control your system swap
     behavior. It's define how memory pages are swapped to disk. A high
     value of <emphasis>swappiness</emphasis> result in a more swapping
     system. Value available are from <option>0</option> to
     <option>100</option>. A value of <option>100</option> told the system
     to find inactive pages and put them in swap.
    </para>
    <para>
     To change the value and do some experiment on a live system, you just
     need to do an echo of the value, and check you system memory usage (ie:
     with the <command>free</command> command):
    </para>
<screen># echo 25 > /proc/sys/vm/swappiness</screen>
<screen># free
             total       used       free     shared    buffers     cached
Mem:      24616680    4991492   19625188     167056     144340    2152408
-/+ buffers/cache:    2694744   21921936
Swap:      6171644          0    6171644</screen>
    <para>
     To get this permanently add a line in
     <filename>/etc/systcl.conf</filename>:
    </para>
<screen>vm.swappiness = 35</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="vt.best.perf.io">
   <title>I/O Scheduler</title>
   <para>
    The default I/O scheduler is the Completely Fair Queuing (CFQ). The main
    aim of CFQ scheduler is to provide a fair allocation of the disk I/O
    bandwidth for all the processes which requests an I/O operation. You can
    have different I/O scheduler for different device.
   </para>
   <para>
    To get better performance in host and &vmguest; it is recommended to use
    <emphasis>noop</emphasis> in the &vmguest; (disable the I/O scheduler)
    and the <emphasis>deadline</emphasis> scheduler for a virtualization
    host.
   </para>
   <sect3>
    <title>Checking current I/O scheduler</title>
    <para>
     To check your current I/O scheduler for your disk (replace
     <replaceable>sdX</replaceable> by the disk you want to check):
    </para>
<screen># cat /sys/block/<replaceable>sdX</replaceable>/queue/scheduler
noop [deadline] cfq</screen>
    <para>
     In our example the <emphasis>deadline</emphasis> is selected.
    </para>
   </sect3>
   <sect3>
    <title>Changing at Runtime</title>
    <para>
     You can change it at runtime with <command>echo</command>:
    </para>
<screen># echo noop > /sys/block/<replaceable>sdX</replaceable>/queue/scheduler</screen>
   </sect3>
   <sect3>
    <title>SLE11 SPX product</title>
    <para>
     To change the value at boot time for SLE11 SPX product, you need to
     modify your <filename>/boot/grub/menu.lst</filename> file, adding
     <option>elevator=deadline</option> for host and
     <option>elevator=noop</option> for &vmguest; (change will be taken into
     account at next reboot). This will be applied to all devices on your
     system.
    </para>
    <para>
     Changing the <option>elevator=</option> for the boot command line will
     apply the <option>elevator</option> to all devices on your system.
    </para>
   </sect3>
   <sect3>
    <title>SLE12 SPX product</title>
    <para>
     To change the value at boot time for SLE12 SPX product, you need to
     modify your <filename>/etc/default/grub</filename> file. Find the
     variable starting with <option>GRUB_CMDLINE_LINUX_DEFAULT</option> and
     add at the end <option>elevator=deadline</option> (or change it with
     the correct value if it is already available).
    </para>
    <para>
     Now you need to regenerate your grub2 config with:
    </para>
<screen># grub2-mkconfig -o /boot/grub2/grub.cfg</screen>
    <para>
     If you want to have a different parameter for each disk just create a
     <filename>/usr/lib/tmpfiles.d/IO_ioscheduler.conf</filename> file with:
    </para>
<screen>w /sys/block/sda/queue/scheduler - - - - deadline
w /sys/block/sdb/queue/scheduler - - - - noop</screen>
   </sect3>
  </sect2>

<!--
  <sect2>
   <title>NFS storage</title>
  <para>
  </para>
  </sect2>
 -->

  <sect2 xml:id="vt.best.perf.disable">
   <title>Disable Unused Tools and Devices</title>
   <para>
    You should minimize software and service available on the hosts.
    Moreover it's not recommended to mix different <emphasis>virtualization
    technologies</emphasis>, like KVM and Containers, on the same host, this
    will reduce resource, will increase security risk and software update
    queue. This lead to reduce the overall host availability, will degrade
    performance even if each resource for both technologies are well sized.
   </para>
   <para>
    Most of Operating System default installation configuration are not
    optimized for VM usage. You should only install what you really need,
    and remove all other components in &vmguest;.
   </para>
   <para>
    Windows Guest:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Disable the screensaver
     </para>
    </listitem>
    <listitem>
     <para>
      Remove all graphical effects
     </para>
    </listitem>
    <listitem>
     <para>
      Disable indexing of hard drive disk if not necessary
     </para>
    </listitem>
    <listitem>
     <para>
      Check the list of started services and disable the one you don't need
     </para>
    </listitem>
    <listitem>
     <para>
      Check and remove all devices not necessary
     </para>
    </listitem>
    <listitem>
     <para>
      Disable system update if not needed or configure it to avoid any delay
      while rebooting or shutting down the host
     </para>
    </listitem>
    <listitem>
     <para>
      Check the Firewall rules
     </para>
    </listitem>
    <listitem>
     <para>
      Schedule appropriately backups and anti-virus programs
     </para>
    </listitem>
    <listitem>
     <para>
      Install
      <link xlink:href="https://www.suse.com/products/vmdriverpack/">VMDP</link>
      para-virtualized driver for best performance
     </para>
    </listitem>
    <listitem>
     <para>
      Check the operating System recommendation like in
      <link xlink:href="http://windows.microsoft.com/en-us/windows/optimize-windows-better-performance#optimize-windows-better-performance=windows-7">Microsoft
      Windows 7 better performance</link> web page
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Linux Guest:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Remove Xorg start if not needed
     </para>
    </listitem>
    <listitem>
     <para>
      Check the list of started services and adjust accordingly
     </para>
    </listitem>
    <listitem>
     <para>
      Operating system needs specific kernel parameters for best
      performance, check the OS recommendations
     </para>
    </listitem>
    <listitem>
     <para>
      Restrict installation of software to a minimal fingerprint
     </para>
    </listitem>
    <listitem>
     <para>
      Optimize the scheduling of predictable tasks (system update, hard
      drive disk checking etc...)
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="vt.best.perf.mtype">
   <title>Updating the Guest Machine Type</title>
   <para>
    &qemu; machine types define some details of the architecture which are
    particularly relevant in the context of migration and save/restore,
    where all the details of the virtual machine ABI need to be carefully
    accounted for. As changes or improvements to &qemu; are made or certain
    types of fixes are done, new machine types are created which include
    those changes. Though the older machine types used for supported
    versions of &qemu; are still valid to use, the user would do well to try
    to move to the latest machine type supported by the current release, so
    as to be able to take advantage of all the changes represented in that
    machine type.
   </para>
   <para>
    Changing the guest's machine type for a Linux guest will mostly be
    transparent, whereas for Windows* guests, it is probably a good idea to
    take a snapshot or backup of the guest in case Windows* has issues with
    the changes it detects and subsequently the user decides to revert to
    the original machine type the guest was created with.
   </para>
   <note>
    <title>Changing the Machine Type</title>
    <para>
     Please refer to the Virtualization guide section
     <link xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_virt/book_virt.html#sec.libvirt.config.mahcinetype.virsh">Change
     Machine type</link> for documentation.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="vt.best.perf.cpu">
   <title>Understanding CPU</title>
   <para>
    Allocation of resources for &vmguest; is a crucial point in VM
    administration. Each &vmguest; should be <emphasis>sized</emphasis> to
    be able to run a certain amount of services, but over-allocating
    resources for &vmguest; may impact the host and all other &vmguest;s. If
    all &vmguest;s suddenly requested all their resources, the host won't be
    able to provide all of them, and this will impact the host's performance
    and will degrade all other services running on the host.
   </para>
   <para>
    CPU's Host <emphasis>components</emphasis> will be
    <emphasis>translated</emphasis> as vCPU in the &vmguest;, but even if
    you have a multi-core CPU with Hyper threading, you should understand
    that a main CPU unit and a multi-core and Hyper threading doesn't
    provide the same computation capabilities:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <emphasis>CPU processor</emphasis>: it describes the main CPU unit, it
      can be multi-core and Hyper threaded, and most of dedicated server
      have multi CPU processor on their motherboard.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>CPU core</emphasis>: a main CPU unit can provide more than
      one core, proximity of cores speed up computation process and reduce
      energy cost.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>CPU Hyper threading</emphasis>: this implementation is used
      to improve parrallelization of computations, but this is not efficient
      as a dedicated core.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="vt.best.perf.cpuparam">
   <title>CPU Parameter</title>
   <para>
    You should avoid CPU over-commit. Unless you know exactly how many vCPU
    you require for your &vmguest; you should start with 1 vCPU per
    &vmguest;. Each vCPU should match one hardware processor or core. You
    should target a workload of 70% inside your VM (could be checked with a
    lot of tools like the <command>top</command>). If you allocate more
    processor than needed in the VM, this will add overhead, and will
    degrade cycle efficiency, the un-used vCPU will consume timer interrupts
    and will idle loop, then this will impact the &vmguest;, but also the
    host. To optimize the performance usage it's recommended to know if your
    applications are single threaded or not to avoid any over-allocation of
    vCPU.
   </para>
   <sect3>
   <title>vCPU model and Features</title>
   <para>
    <emphasis>vCPU model and features</emphasis>: CPU model and topology
    can be specified for each &vmguest;. The vCPU definition could be very
    specific excluding some CPU features, listing exact one, etc... You
    can use predefined models available in
    <filename>/usr/share/libvirt/cpu_map.xml</filename> file to exactly
    match your need. Event if could be interesting to declare a very
    specific vCPU for a &vmguest; you should keep in mind that's normalize
    vCPU model and features simplify migration among heterogeneous hosts
    (see
    <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_libvirt_admin_migrate.html">libvirt
    migration guide</link>). Because change the vCPU type require that the
    &vmguest; is off, which is a constraint in a production environment.
    You should also consider that multiple sockets with a single core and
    thread generally give best performance.
   </para>
   <para>To get all capabilities and topologies available on your system use <command>virsh capabilities</command> command.</para>
   </sect3>
   <sect3>
    <title>vCPU Pinning</title>
    <para>
     <emphasis>vCPU Pinning</emphasis>: it's a method to constrain vCPU
     threads to a NUMA node. The <emphasis>vcpupin</emphasis> element
     specifies which of host's physical CPUs the domain vCPU will be pinned
     to. If this is omitted, and attribute <emphasis>cpuset</emphasis> of
     element <emphasis>vcpu</emphasis> is not specified, the vCPU is pinned
     to all the physical CPUs by default.
    </para>
    <para>
     You can pin a vCPU to a specific physical CPU. vCPU will increase CPU cache hit ratio. To pin a vCPU to a specific core:
    </para>
    <screen>virsh# vcpupin <replaceable>DOMAIN</replaceable> --vpcu <replaceable>vCPU_NUMBER</replaceable>
VCPU: CPU Affinity
----------------------------------
   0: 0-7
virsh # vcpupin SLE12 --vcpu 0 0 --config</screen>
   <para>in The XML configuration of your domain now you should have:</para>
   <screen>&lt;cputune&gt;
    &lt;vcpupin vcpu='0' cpuset='0'/&gt;
&lt;/cputune&gt;</screen>
   </sect3>
   <sect3>
    <title>libvirt and CPU Configuration</title>
    <para>
     For more information about vCPU configuration and tuning parameter
     please refer to the
     <link xlink:href="https://libvirt.org/formatdomain.html#elementsCPU">libvirt</link>
     documentation.
    </para>
   </sect3>
  </sect2>
  
  <sect2 xml:id="vt.best.perf.numa">
   <title>NUMA affinity</title>
   <para>
    Potentially using <emphasis>NUMA</emphasis> huge impact on performance. You should consider your host topology when sizing guests. First check that your host has NUMA capabilities:
   </para>
   <screen># numactl --hardware
available: 4 nodes (0-3)
node 0 cpus: 0 1
node 0 size: 2047 MB
node 0 free: 317 MB
node 1 cpus: 2 3
node 1 size: 2047 MB
node 1 free: 6 MB
node 2 cpus: 4 5
node 2 size: 2048 MB
node 2 free: 132 MB
node 3 cpus: 6 7
node 3 size: 2048 MB
node 3 free: 582 MB
node distances:
node   0   1   2   3 
  0:  10  20  20  20 
  1:  20  10  20  20 
  2:  20  20  10  20 
  3:  20  20  20  10</screen>
  <para>
   There is 3 memory cpuset policy mode available: interleave bind prefered. 
  </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>interleave</emphasis>: is a memory placement policy also know as round-robin. 
      This policy can provide substantial improvements for jobs that need
      to place thread local data on the corresponding node. When the interleave destination
      is not available, this will be move to another nodes.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>bind</emphasis>: this will place memory only on one node, which means in case of insuficient memory, the allocation will fail.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>prefered</emphasis>: this policy will put a preference to allocate a
      memory to a node, but if there is not enough place for memory on this node, this will
      fallback to another node.
    </para>
    </listitem>
   </itemizedlist>
   <para>
    Now you should found where the &vmguest; has allocated his pages with <command>cat /proc/<replaceable>PID</replaceable>/numa_maps</command> and <command>cat /sys/fs/cgroup/memory/sysdefault/libvirt/qemu/<replaceable>KVM_NAME</replaceable>memory.numa_stat</command>. You should avoid allocating &vmguest; memory across NUMA nodes, and prevent vCPUs from floating across NUMA nodes.
   </para>
   <para>
    You can change memory policy mode with <command>cgset</command> tool:
   </para>
   <screen># cgset -r cpuset.mems=<replaceable>NODE</replaceable> sysdefault/libvirt/qemu/<replaceable>KVM_NAME</replaceable>/emulator</screen>
   <para>To migrate pages to a node use the <command>migratepages</command> tool:</para>
   <screen># migratepages <replaceable>PID</replaceable> <replaceable>FROM-NODE</replaceable> <replaceable>TO-NODE</replaceable></screen>
   <para>to check everything is fine <command>cat /proc/<replaceable>PID</replaceable>/status | grep Cpus</command>.</para>   
   <note>
    <title>Kernel NUMA/cpuset Memory Policy</title>
    <para>
     For more information see <link xlink:href="https://www.kernel.org/doc/Documentation/vm/numa_memory_policy.txt">Kernel NUMA memory policy</link> and <link xlink:href="https://www.kernel.org/doc/Documentation/cgroups/cpusets.txt">cpusets memory policy</link>.
    </para>
   </note>
  </sect2>

<!-- TODO
      <sect2 id="vt.best.perf.mem">
      <title>Memory</title>
      <para>
      Hugepages
      </para>
      <para>
      HugeTLB
      </para>
      </sect2>
 -->
 </sect1>
 <sect1 xml:id="vt.best.stor">
  <title>Storage and Filesystem</title>
  <para>
  </para>

<!--
 <sect2 id="vt.best.stor.general">
  <title>General</title>
  <para>
   Access CD/DVD -> storage pool
  </para>
  <para>
   deleting pool
  </para>
  <para>
   Brtfs and guest image
  </para>
  <para>
   SCSI Host Adapter name (avoid /dev/sdX)
  </para>
  <para>
   qemu direct access to host drives (-drive file=)
  </para>
  <para>
   QEMU Storage formats
  </para>
 </sect2>
 <sect2 id="vt.best.stor.blkvsimg">
  <title>BLK VS Images files</title>
  <para>
   you can use block devices or files as local storage devices within guest
   operating systems.
  </para>

  <para>
  Block devices:
  </para>
  <itemizedlist>
   <listitem><para>
    Better performance
   </para></listitem>
   <listitem><para>
    Use “standard” tools for administration/disk modification
    </para></listitem>
    <listitem><para>
     Accessible from host (pro and con)
    </para></listitem>
  </itemizedlist>
  <para>
   Image Files
  </para>
  <itemizedlist>
   <listitem><para>
    Easier system management
   </para></listitem>
   <listitem><para>
    Easily move, clone, backup domains
   </para></listitem>
   <listitem><para>
    Comprehensive toolkit (guestfs) for image manipulation
    </para></listitem>
    <listitem><para>
     Reduce overhead through sparse files
    </para></listitem>
    <listitem><para>
     Fully allocate for best performance
    </para></listitem>
  </itemizedlist>
 </sect2>
-->

  <sect2 xml:id="vt.best.stor.imageformat">
   <title>&vmguest; Image Format</title>
   <para>
    Certain storage formats which &qemu; recognizes have their origins in
    other virtualization technologies. By recognizing these formats, &qemu;
    is able to leverage either data stores or entire guests which were
    originally targeted to run under these other virtualization
    technologies. Some of these formats are supported only in read-only
    mode, enabling either direct use of that read only data store in a
    &qemu; guest or conversion to a fully supported &qemu; storage format
    (using <command>qemu-img</command>) which could then be used in
    read/write mode. See &sle;
    <link xlink:href="https://www.suse.com/releasenotes/x86_64/SUSE-SLES/12/#fate-317891">Release
    Notes</link> to get the list of supported format.
   </para>
   <note>
     <para>
       It is recommended to convert the disk images to either
       raw or qcow2 in order to achieve good performance.
     </para>
   </note>
   <warning>
     <title>Encryption</title>
     <para>
       When you create an image, you can not use compression (<option>-c</option>) in the output file
       with the encryption option (<option>-e</option>).
     </para>
   </warning>
   <sect3>
    <title>Raw format</title>
    <para></para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
       <para>
	 This format is simple and easily exportable to all other emulators/hypervisors
       </para>
     </listitem>
     <listitem>
      <para>
	It provides best performance (least I/O overhead)
      </para>
     </listitem>
     <listitem>
       <para>
	 If your file system supports holes (for example in ext2 or ext3 on
	 Linux or NTFS on Windows), then only the written sectors will
	 reserve space
     </para>
     </listitem>
     <listitem>
       <para>
	 The raw format permit to copy a &vmguest; image to a physical device
	 (<command>dd if=<replaceable>vmguest.raw</replaceable> of=<replaceable>/dev/sda</replaceable></command>)
       </para>
     </listitem>
     <listitem>
       <para>
	 It's byte-by-byte the same as what the &vmguest; sees, so this waste
	 a log of space
       </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3>
     <title>qcow2 format</title>
     <para></para>
     <itemizedlist mark="bullet" spacing="normal">
       <listitem>
      <para>
	Use it to have smaller images (useful if your filesystem does not supports holes, for example on
	Windows)
      </para>
     </listitem>
     <listitem>
       <para>
	 It has an optional AES encryption
       </para>
     </listitem>
     <listitem>
       <para>
	 zlib based compression option
       </para>
     </listitem>
     <listitem>
       <para>
	 Support of multiple VM snapshots (internal, external)
       </para>
     </listitem>
     <listitem>
       <para>
	Improved performance and stability
      </para>
     </listitem>
      <listitem>
	<para>
	  support changing the backing file
	</para>
      </listitem>
      <listitem>
	<para>
	  Support consistency checks
	</para>
      </listitem>
      <listitem>
        <para>less performance than raw format
	</para>
      </listitem>
     </itemizedlist>
      <sect4>
	<title>Cluster Size</title>
	<para>qcow2 format offer the capabilities to change the cluster
	size. Thee value must be between 512 and 2M. Smaller cluster sizes can
	improve the image file size whereas larger cluster sizes
        generally provide better performance.</para>
      </sect4>
      <sect4>
	<title>Pre allocation</title>
	<para>
	  An image with preallocated metadata is initially larger but can
	  improve performance when the image needs to grow.
	</para>
      </sect4>
      <sect4>
      	<title>Lazy Refcounts</title>
	<para>Reference count updates are postponed with the goal of
	avoiding metadata I/O and improving performance. This is
        particularly interesting with <option>cache=writethrough</option> which doesn't
	batch metadata updates. The tradeoff is that after a host crash, the
	reference count tables must be rebuilt (On the next open an
	(automatic) 
	<command>qemu-img check -r all</command> is required, which may take
	some time.</para>
      </sect4>
   </sect3>
    <sect3>
      <title>qed format</title>
      <para>
	qed is the next generation qcow.
      </para>
      <itemizedlist>
	<listitem>
	  <para>
	    Strong data integrity due to simple design 
	  </para>
	</listitem>
	<listitem>
	  <para>
	    Retains sparseness over non-sparse channels (e.g. HTTP) 
	 </para>
       </listitem>
       <listitem>
	 <para>
	   support changing the backing file
	 </para>
       </listitem>
       <listitem>
	 <para>
	   Support consistency checks
	 </para>
       </listitem>
       <listitem>
         <para> 
           Fully asynchronous I/O path 
	 </para>
       </listitem>  
       <listitem>
         <para>
           doesn't support internal snapshot
         </para>
       </listitem>
       <listitem>
         <para>
	   relies on the host file system and cannot be stored on a logical volume directly
	 </para>
       </listitem>
     </itemizedlist>
   </sect3>
   <sect3>
     <title>VMDK format</title>
     <para>
       VMware 3, 4, or 6 image format, for exchanging images with that product.
     </para>
   </sect3>
   <sect3>
     <title>Image Information</title>
     <para>
       Use <command>qemu-img info <replaceable>vmguest.img</replaceable></command> to get images's information like:
       the format, the virtual size, the physical size, snapshots if available.
     </para>
   </sect3>
   <sect3>
     <title>qemu-img Reference</title>
     <para>
       Please refer to <link
       xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_virt/book_virt.html#cha.qemu.guest_inst.qemu-img.create">SLE12
       qemu-img documentation</link> for more information on <command>qemu-img</command> tool and examples.
     </para>
     </sect3>
     <sect3>
       <title>Overlay Storage Image</title>
       <para>
	 The qcow2 and qed formats provide a way to create a base-image, but also a way
	 to create available overlay disk images on top of the
	 base image. Backing file is useful to be able to revert to a know
	 state and discard the overlay.
       </para>
       <para>
	 To create an overlay image:
       </para>
       <screen> qemu-img create -o<co xml:id="co.1.minoro"/>backing_file=vmguest.raw<co xml:id="co.1.backingfile"/>,backing_fmt=raw<co
       xml:id="co.1.backingfmt"/>\
       -f<co xml:id="co.1.minorf"/> qcow2 vmguest.cow<co xml:id="co.1.imagename"/></screen>
       <calloutlist>
	 <callout arearefs="co.1.minoro">
           <para>a comma separated list of format specific options in a
	   <option>name=value</option> format. Use <option>-o ?</option> for an overview of the options</para>
         </callout>
	 <callout arearefs="co.1.backingfile">
	   <para>the backing file name</para>
	 </callout>
	 <callout arearefs="co.1.backingfmt">
	   <para>specify the file format for the backing file</para>
	 </callout>
	 <callout arearefs="co.1.minorf">
	   <para>specify the image format for the &vmguest;</para>
	 </callout>
	 <callout arearefs="co.1.imagename">
	   <para>image name of the &vmguest;, it will only record the
	   differences from the backing_file</para>
	 </callout>
       </calloutlist>
       <para>
	 Now you can start your &vmguest;, use it do some modification
	 etc.... the backing image will be untouched and all changes to the
	 storage will be recorded in the overlay image file. The backing file will
	 never be modified unless you use the <option>commit</option> monitor command (or
	 <command>qemu-img commit</command>).
       </para>
       <warning>
	 <title>Backing Image Path</title>
	 <para>
	   You should not change the path to the backing image, or you need to
	   adjust it. The path is stored is the overlay image file. If you
	   want to update the path, you should do a symbolic link from
	   original path to the new path and then use the
	   <command>qemu-img</command> <option>-rebase</option> option.
	   <screen># ln -sf /var/lib/images/vmguest.raw  /var/lib/images/SLE12/vmguest.raw</screen>
	   <screen># qemu-img rebase<co xml:id="co.2.rebase"/> -b<co xml:id="co.2.minorb"/> /var/lib/images/vmguest.raw /var/lib/images/SLE12/vmguest.cow<co xml:id="co.2.image"/></screen>
	   <calloutlist>
	     <callout arearefs="co.2.rebase">
	       <para>changes the backing file image
	       </para>
	     </callout>
	     <callout arearefs="co.2.minorb">
	       <para>specify the backing file image to use</para>
	     </callout>
	     <callout arearefs="co.2.image">
               <para>specify the image taht will be affected</para>
             </callout>
	   </calloutlist>
	 </para>
	 <note>
	   <title>Safe or Unsafe rebase</title>
	   <para></para>
	 </note>
       </warning>
       <para>
	 A current usage is to initiate a new guest with the backing file. Let's
	 assume we have a <filename>sle12_base.img</filename> &vmguest;
	 ready to use (fresh installation without any modification). This will
	 be our backing file.
	 Now you need to test a new package, on an updated system, and on a
	 system with a different kernel.
	 We can use <filename>sle12_base.img</filename> to instance new &sle; &vmguest; by 
	 creating a qcow2 overlay files, pointing to this backing file
	 (<filename>sle12_base.img</filename>).
       </para>
       <para>
	 In our example we will use <filename>sle12_updated.qcow2</filename>
	 for the updated system, and
	 <filename>sle12_updated.qcow2</filename>
	 for the system with a different kernel.
       </para>
       <para>To create the two thin provisionned system use the
       <command>qemu-img</command> command
       line with <option>-b</option> option:</para>
       <screen># qemu-img create -b /var/lib/libvirt/sle12_base.img -f qcow2 \
/var/lib/libvirt/sle12_updated.qcow2
Formatting 'sle12_updated.qcow2', fmt=qcow2 size=17179869184
       backing_file='sle12_base.img' encryption=off cluster_size=65536 
       lazy_refcounts=off nocow=off</screen>
       <screen># qemu-img create -b /var/lib/libvirt/sle12_base.img -f qcow2 \
/var/lib/libvirt/sle12_kernel.qcow2
Formatting 'sle12_kernel.qcow2', fmt=qcow2 size=17179869184
       backing_file='vmguest-sle12_base.img' encryption=off cluster_size=65536 
       lazy_refcounts=off nocow=off</screen>
       <para>
	 The images are now usable, and you can do your test without touching the
	 initial <filename>sle12_base.img</filename> backing file, all
	 change will be stored in the new images. Moreover, you can also use
	 these new images as a backing file, and create a new overlay.
       </para>
       <screen># qemu-img create -b sle12_kernel.qcow2 -f qcow2 sle12_kernel_TEST.qcow2</screen>
       <para>
	 <command>qemu-img info</command> with the option
	 <option>--backing-chain</option> will return all information of the
	 entire backing chain recursively:
       </para>
       <screen>qemu-img info --backing-chain<co xml:id="co.3.backingchain"/> \
       /var/lib/libvirt/images/sle12_kernel_TEST.qcow2
image: sle12_kernel_TEST.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: sle12_kernel.qcow2
Format specific information:
    compat: 1.1
    lazy refcounts: false

image: sle12_kernel.qcow2
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 196K
cluster_size: 65536
backing file: SLE12.qcow2
Format specific information:
    compat: 1.1
    lazy refcounts: false

image: sle12_base.img
file format: qcow2
virtual size: 16G (17179869184 bytes)
disk size: 16G
cluster_size: 65536
Format specific information:
    compat: 1.1
    lazy refcounts: true</screen>
       <calloutlist>
         <callout arearefs="co.3.backingchain">
           <para>will enumerate information about backing files in a disk image chain</para>
         </callout>
       </calloutlist>
       <figure xml:id="fig.qemu-img.overlya">
	 <title>Understanding Image Overlay </title>
	 <mediaobject>
	   <imageobject role="fo">
             <imagedata fileref="qemu-img-overlay.png" width="95%" format="PNG"/>
	   </imageobject>
	   <imageobject role="html">
             <imagedata fileref="qemu-img-overlay.png" width="95%" format="PNG"/>
	   </imageobject>
	 </mediaobject>
       </figure>
     </sect3>
  </sect2>

<!--
 <sect2 id="vt.best.stor.diskio">
  <title>Disk IO Modes</title>
  <table>
   <title>Notation Conventions</title>
   <tgroup cols="2">
    <colspec colnum="1" colname="1"/>
    <colspec colnum="2" colname="2"/>
    <thead>
     <row>
      <entry>
       <para>Mode</para>
      </entry>
      <entry>
       <para>Description</para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>Disk IO Modes
       </para>
      </entry>
      <entry>
       <para>Native</para>
      </entry>
     </row>
     <row>
      <entry>
       <para>kernel asynchronous IO</para>
      </entry>
      <entry>
       <para>threads</para>
      </entry>
     </row>
     <row>
      <entry>
       <para>host user-mode based threads</para>
      </entry>
      <entry>
       <para>default, 'threads' mode in SLES</para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
 </sect2>
-->
 </sect1>
 <sect1>
  <title>&xen;: Moving from PV to FV</title>

  <para>
   This chapter explain how to convert a &xen; para-virtual machine to &xen;
   full virtualized machine.
  </para>

  <para>
   First you need to change to the <emphasis>-default</emphasis> kernel, if
   not already installed, you must install it while in PV mode.
  </para>

  <para>
   In case of you are using <emphasis>vda*</emphasis> naming for disk, you
   need to change this to <emphasis>hd*</emphasis> in
   <filename>/etc/fstab</filename>, <filename>/boot/grub/menu.lst</filename>
   and <filename>/boot/grub/device.map</filename>.
  </para>

  <note>
   <title>Prefer UUIDs</title>
   <para>
    You should use UUIDs or Logical Volumes within your
    <filename>/etc/fstab</filename>. Usage of UUID sumplify attached network
    storage, multipathing, and virtualization.
   </para>
  </note>

  <para>
   Moving from PV to FV will lead to a missing disk driver modules from the
   initrd. The modules expected is <emphasis>xen-vbd</emphasis> (and
   <emphasis>xen-vnif</emphasis> for network). They are the PV drivers for
   fully &vmguest;. All other modules like <emphasis>ata_piix</emphasis>,
   <emphasis>ata_generic</emphasis> and <emphasis>libata</emphasis> should
   be added automatically.
  </para>

  <para>
   With SLES11, you can add modules in the
   <emphasis>INITRD_MODULES</emphasis> line in the
   <filename>/etc/sysconfig/kernel</filename> file. Run
   <command>mkinitrd</command> to add them to the initrd.
  </para>

  <para>
   You need to change few parameter in the XML config file which describes
   your &vmguest;:
  </para>

  <para>
   Set the OS section to something like:
  </para>

<screen>&lt;os&gt;
    &lt;type arch='x86_64' machine='xenfv'&gt;hvm&lt;/type&gt;
    &lt;loader>/usr/lib/xen/boot/hvmloader&lt;/loader&gt;
    &lt;boot dev='hd'/&gt;
&lt;/os&gt;</screen>

  <para>
   In the devices section, you need to add:
  </para>

<screen>&lt;emulator&gt;/usr/lib/xen/bin/qemu-system-i386&lt;/emulator&gt;</screen>

  <para>
   Replace the <emphasis>xen</emphasis> disk bus with
   <emphasis>ide</emphasis>, and the <emphasis>xvda</emphasis> target device
   with <emphasis>hda</emphasis>.
  </para>

  <note>
   <title>guestfs-tools</title>
   <para>
    If you want to script this process, or work on disk images directly, you
    can use the
    <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_guestfs_tools.html">guestfs-tools</link>
    suite as numerous tools exist there to help to modify disk images.
   </para>
  </note>
 </sect1>
<!--
<sect1 id="vt.best.snapsav">
 <title>Saving, Migrating and Snapshoting</title>
 <para>
  Migration requirements/Recomendations
  save VM and start/boot (memory invalid)
  snapshot naming importance
  avoid qemu-img snapshot
  cache mode in live migration
  guestfs and live system
 </para>
</sect1>


<sect1 id="vt.best.security">
 <title>Security consideration</title>
 <para>
  Connection to guest: security policy
  Authentication for libvirtd and VNC need to be configured separately
  RNG and entropy
  &qemu; Guest Agent
  The VNC TLS (set at start)
 </para>
</sect1>

<sect1 id="vt.best.pcipass">
 <title>pcpipass</title>
 <para>
  Pci device (not online!, managed/unmanaged)
  howto check SR-IOV capabilities
 </para>
</sect1>
-->
<!--
<sect1 xml:id="vt.best.net">
 <title>Network Tips</title>
 <para>
 </para>

 <sect2 xml:id="vt.best.net.vnic">


   <title>Virtual NICs</title>
  <para>
   virtio-net (KVM) : multi-queue option
   vhost-net (KVM) : Default vNIC, best performance
   netbk (Xen) : kernel threads vs tasklets
  </para>
 </sect2>

  <sect2 xml:id="vt.best.net.enic">
   <title>Emulated NICs</title>
   <para>
   e1000: Default and preferred emulated NIC
   rtl8139
   </para>
  </sect2>

  <sect2 xml:id="vt.best.net.sharednic">
   <title>Shared Physical NICs</title>
   <para>
   SR-IOV: macvtap
   Physicial NICs : PCI pass-through
   </para>
  </sect2>

  <sect2 xml:id="vt.best.general">
   <title>Network General</title>
   <para>
   use multi-network to avoid congestion
   admin, storage, migration ...
   use arp-filter to prevent arp flux

   same MTU in all devices to avoid fragmentation
   yast to configure bridge
   Network MAC address
   bridge configuration in bridge.conf file
   PCI pass-through Vfio to improve performance
   </para>
  </sect2>
 </sect1>
-->
<!--
<sect1 xml:id="vt.best.debug">
 <title>Troubleshooting/Debugging</title>
 <para>
 </para>

 <sect2 xml:id="vt.best.debug.xen">
   <title>Xen</title>
  <para>
  </para>
  <sect3 xml:id="vt.best.debug.xen.log">
   <title>Xen Log Files</title>
   <para>
    libxl logs:
    <filename>/var/log/xen/*</filename>
    qemu-dm-domain.log, xl-domain.log
    bootloader.log, vm-install, xen-hotplug
    Process specific logs, often requiring debug log levels to be useful
    Some logs require 'set -x' to be added to /etc/xen/scripts/*

    libvirt logs:
    <filename>/var/log/libvirt/libxl</filename>
    libxl-driver.log
    domain.log
    </para>
   </sect3>
   <sect3 xml:id="vt.best.debug.xen.hypervisor">
    <title>Daemon and Hypervisor Logs</title>
    <para>
     View systemd journal for specific units/daemons: <command>journalctl
    <command>journalctl [\-\-follow] –unit xencommons.service</command>
    <command>journalctl /usr/sbin/xenwatchdogd</command>
    xl dmesg
    Xen hypervisor logs
    </para>
   </sect3>

  <sect3 xml:id="vt.best.debug.xen.loglevel">
    <title>Increasing Logging Levels</title>
    <para>
     Log levels are increased through xen parameters:
    </para>
    <screen>loglvl=all</screen>
    <para>
    Increased logging for Xen hypervisor
    </para>
    <screen>guest_loglvl=all</screen>
    <para>
     Increased logging for guest domain actions Grub2 config:
   
    Edit <filename>/etc/default/grub</filename>, then recreate <filename>grub.cfg</filename>:
    GRUB_CMDLINE_XEN_DEFAULT=”loglvl=all guest_loglvl=all”
    <command>grub2-mkconfig -o /boot/grub/grub.cfg</command>
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="vt.best.debug.support">
   <title>Support</title>
  <para></para>
   <sect3 xml:id="vt.best.debug.support.config">
    <title>Supportconfig and Virtualization</title>
    <para>
    Core files:
    basic-environment.txt
    Reports detected virtualization hypervisor
    Under some hypervisors (xen), subsequent general checks might be incomplete
    
    Hypervisor specific files:
    kvm.txt, xen.txt
    Both logs contain general information:
    RPM version/verification of important packages
    Kernel, hardware, network details
    </para>
   </sect3>
  
  <sect3 xml:id="vt.best.debug.support.kvm">
    <title>kvm.txt</title>
    <para>
    libvirt details
    General libvirt details
    Libvirt daemon status
    KVM statistics
    virsh version, capabilities, nodeinfo, etc...
    
    Domain list and configurations
    Conf and log files
    <filename>/etc/libvirt/libvirtd.conf</filename>
    Last 500 lines from <filename>/var/log/libvirt/qemu/domain.log</filename>
    </para>
   </sect3>

  <sect3 xml:id="vt.best.debug.support.xen">
   <title>xen.txt</title>
    <para>
    Daemon status
    xencommons, xendomains and xen-watchdog daemons
    grub/grub2 configuration (for xen.gz parameters)

    libvirt details
    Domain list and configurations
    
    xl details
    Domain list and configurations
    Conf and Log files
    <filename>/etc/xen/xl.conf</filename>, <filename>/etc/libvirt/libvirtd.conf</filename>
    Last 500 lines from <filename>/var/log/xen/*</filename>,
    <filename>/var/log/libvirt/libxl/*</filename> 
    Output of <command>xl dmesg</command> and <command>xl info</command>
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="vt.best.debug.advanced">
   <title>Advanced Debugging Options</title>
   <para>
    Serial console
   </para>
   <screen>GRUB_CMDLINE_XEN_DEFAULT=“loglvl=all guest_loglvl=all console=com1 com1=115200,8n1”</screen>
   <screen>GRUB_CMDLINE_LINUX_DEFAULT=“console=ttyS0,115200”</screen>
   <para>
    Debug keys
    <command>xl debug keys h; xl dmesg</command>
    <command>xl debug keys q; xl dmesg</command>
   Additional Xen debug tools:
   <command>xenstore-{ls,read,rm,watch,write}, xentrace, xentop</command>
   
   Capturing Guest Logs
   Capturing guest logs during triggered problem:
   Connect to domain:
   <command>virsh console domname</command>
   Execute problem command
   Capturing domain boot messages
   </para>
   <screen>xl create -c VM config file</screen>
   <screen>virsh create VM config file \-\-console</screen>
 </sect2>
 <sect2 xml:id="vt.best.trouble">
   <title>Troubleshooting Installations</title>
   <para>
   virt-manager and virt-install logs:
   Found in <filename>~/.cache/virt-manager</filename>

   Debugging virt-manager:
   <command>virt-manager \-\-no-fork</command>
   Sends messages directly to screen and log file
  </para>
   <screen>LIBVIRT_DEBUG=1 virt-manager \-\-no-fork</screen>
   <para>
   See libvirt messages in <filename>/var/log/messages</filename>

   Use <command>xl</command> to rule out libvirt layer
  </para>
 </sect2>
 <sect2 xml:id="vt.best.trouble.libvirt"> 
   <title>Troubleshooting Libvirt</title>
   <para>
    Client side troubleshooting
   </para>
   <screen>LIBVIRT_DEBUG=1
   1: debug, 2: info, 3: warning, 4: error</screen>
   <para>
   Server side troubleshooting
   <filename>/etc/libvirt/libvirtd.conf</filename> (restart libvirtd after changes)
   log_level = 1
   log_output = “1:file:/var/log/libvirtd.log”
   log_filters = “1:qemu 3:remote”
  </para>
 </sect2>

 <sect2 xml:id="vt.best.trouble.kernel">
  <title>Kernel Cores</title>
  <para>
   Host cores -vs- guest domain cores
   Host cores are enabled through Kdump YaST module
   For Xen dom0 cores, 'crashkernel=size@offset' should be added as a Xen hypervisor parameter

   Guest cores require:
   on_crash[action]on_crash tag
   Possible coredump actions are:
   coredump-restart     Dump core, then restart the VM
   coredump-destroy    Dump core, then terminate the VM
   Crashes are written to:
   <filename>/var/lib/libvirt/{libxl,qemu}/dump</filename>
   <filename>/var/lib/xen/dump</filename>  (if using xl).
  </para>
 </sect2>


 <sect2 xml;id="vt.best.debug.other">
  <title>Other</title>
  <para>
   VGA trouble debug
  </para>
 </sect2>
</sect1>
-->
<!--
<sect1>
 <title>References</title>
 <para>
 </para>
  <itemizedlist>
   <listitem><para>
    <link xlink:href="kernel.org/doc/ols/2009/ols2009-pages-19-28.pdf">Increasing memory density using KSM</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="http://www.linux-kvm.org/page/KSM">linux-kvm.org KSM</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="https://www.kernel.org/doc/Documentation/vm/ksm.txt">KSM's kernel documentation</link></para>
   </listitem>
   <listitem>
    <para><link xlink:href="https://lwn.net/Articles/329123/">ksm - dynamic page sharing driver for linux v4</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="http://www.espenbraastad.no/post/memory-ballooning/">Memory Ballooning</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="http://wiki.libvirt.org/page/Virtio">libvirt virtio</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="https://www.kernel.org/doc/Documentation/block/cfq-iosched.txt">CFQ's kernel documentation</link></para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href=""></link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href=""></link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href=""></link>
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
-->
</article>
