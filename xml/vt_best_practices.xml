<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>

<article id="article.vt.best.practices" lang="en">
 <?suse-quickstart color="suse"?>
 <title>Virtualization Best Practices</title>
 <subtitle>&sle; &productname; &productnumber;</subtitle>
 <articleinfo>
  <productnumber>&productnumber;</productnumber><productname>&sle;
  &productname;</productname>
 </articleinfo>

<!--
we can write it based on on scenario?
Virtualization Capabilities:
Consolidation (hardware1+hardware2 -> hardware)
 Isolation
 Migration (hardware1 -> hardware2)
 Disaster recovery
 Dynamic load balancing
-->

<sect1 id="vt.best.scenario">
 <title>Virtualization Scenarios</title>
 <para>
  Virtualization offers a lot of capabilities to your environment.
  It can be used in multiple sort of scenario. To get more details about 
  <ulink url="https://www.suse.com/documentation/sles-12/book_virt/data/sec_virtualization_introduction_capabilities.html">Virtualization Capabilities</ulink> and <ulink
  url="https://www.suse.com/documentation/sles-12/book_virt/data/sec_virtualization_introduction_benefits.html">Virtualization Benefits</ulink> please refer to the <ulink url="https://www.suse.com/documentation/sles-12/book_virt/data/book_virt.html">Virtualization Guide</ulink>.
 </para>
 <para>This best practice will provide you some advice to help you do the
 right choice in your environment, it will recommend or discourage the usage of
 options depending on your usage.</para>

<!-- TODO
 <table rowsep="1">
  <title>Scenario</title>
  <tgroup cols="3">
   <colspec colnum="1" colname="1" colwidth=""/>
   <colspec colnum="2" colname="2" colwidth=""/>
   <thead>
    <row>
     <entry>
      <para>Scenarios</para>
     </entry>
     <entry>
      <para>Option Recommended for</para>
     </entry>
     <entry>
      <para>Option Not recommended for</para>
     </entry>
    </row>
   </thead>
   <tbody>
    <row>
     <entry>
      <para>Consolidation</para>
     </entry>
     <entry><para>X</para></entry>
    </row>
    <row>
     <entry>
      <para>Isolation</para>
     </entry>
     <entry></entry>
     <entry><para>X</para></entry>
    </row>
    <row>
     <entry>
      <para>Migration</para>
     </entry>
     <entry><para>X</para></entry>
    </row>
    <row>
     <entry>
      <para>Disaster Recovery</para>
     </entry>
     <entry>
      <mediaobject>
       <imageobject>
        <imagedata fileref="thumbup_green.png" width="25%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </entry>
    </row>
    <row>
     <entry>
      <para>Dynamic Load Balancing</para>
     </entry>
    <entry></entry>
    <entry>
      <mediaobject>
       <imageobject>
        <imagedata fileref="thumbdown_red.png" width="25%" format="PNG"/>
       </imageobject>
      </mediaobject>
    </entry>
    </row>
   </tbody>
  </tgroup>
 </table>
-->
</sect1>

<sect1 id="vt.best.intro">
 <title>Before Any Modification or Online Production</title>
 <para>
  The efficiency of virtualization environment depends on
  administration choice. This guide is provided as a reference for
  doing good choice in production environment. Nothing is <emphasis>graved on
  stone</emphasis>, and different infra-structure can provide different
  result. <emphasis>Pre-experimentation</emphasis> is a key point to get a successfully
  virtualization environment.
 </para>
 
 <sect2 id="vt.best.intro.backup">
  <title>Backup First</title>
  <para>
   Playing with &vmguest; and the Host configuration can lead to data loss or
   unstable state. It's really important that you do backups of files,
   data, images etc.. before doing any change. Without backups you won't be able
   to easily restore the original state after a data loss or a
   miss-configuration.
  </para>
  <warning>
   <para>
    Backup is mandatory before doing any tests to be sure you will be able to roll back
    to a usable/stable system or configuration.
   </para>
  </warning>
 </sect2>
 
 <sect2 id="vt.best.intro.acpi">
  <title>Do ACPI Testing</title>
  <para>
   The capabilities to change a &vmguest; state heavily depends on the
   operating system. It's really important to test this features before
   any use of you &vmguest; in production. For example most of Linux OS
   disable this capabilities per default, so this require that you enable
   this operation (mostly through Policy Kit). 
  </para>
  <para>
   ACPI must be enabled in the guest for a graceful shutdown to work. 
   To check if ACPI is enabled, run:
  </para>
  <screen>virsh dumpxml <replaceable>VMNAME</replaceable> | grep acpi</screen>
  <para>
   If nothing is printed, ACPI is not enabled for your machine. Use <command>virsh edit</command>
   to add the following XML under &lt;domain&gt;:
  </para>
  <screen>&lt;features&gt;
  &lt;acpi/&gt;
&lt;/features&gt;</screen>
  <para>
   If ACPI was enabled during a Windows Server 20XX guest installation,
   turning it on in the &vmguest;
   configuration alone is not sufficient. See the following articles for
   more information:
  </para>
  <simplelist>
   <member><ulink url="http://support.microsoft.com/kb/314088/EN-US/"/>
   </member>
   <member><ulink url="http://support.microsoft.com/?kbid=309283"/>
   </member>
  </simplelist>
  <para>
   A graceful shutdown is of course always possible from within the guest
   operating system, regardless of the &vmguest;'s configuration.
  </para>
 </sect2>

 <sect2 id="vt.best.intro.libvirt">
  <title>Prefer &libvirt; Framework</title>
  <para>
   In SUSE Linux Enterprise it is recommended to use the &libvirt; framework to
   do any operation on
   hosts, containers and &vmguest;. If you use other tools this may not appear
   or be propagated in you system. For example creating a system image by hand
   with <command>qemu-img <option>create</option> data.raw 10G</command> 
   will not be displayed in the <command>virt-manager</command> pool section. If you use a
   <command>qemu-system-arch</command> command this will no be visible under
   &libvirt;. So you should carefully used any other management tools and keep
   in mind their usage won't be probably reflected on other tools.
  </para>
 </sect2>
 <sect2 id="vt.best.intro.qemu">
  <title>qemu-system-i386 VS qemu-system-x86_64</title>
  <para>
   Just as a modern 64 bit x86 PC supports running a 32 bit OS as well as a 64
   bit OS, <command>qemu-system-x86_64</command> runs 32 bit OS's perfectly fine, and in fact
   usually provides better performance to 32 bit guests than <command>qemu-system-i386</command>,
   which provides a 32 bit guest environment only. Hence we recommend using
   <command>qemu-system-x86_64</command> over
   <command>qemu-system-i386</command> for all guest types. Where
   <command>qemu-system-i386</command>
   is known to perform better is in configurations which SUSE does not support.
  </para>
 </sect2>
</sect1>

<sect1 id="vt.best.guest">
 <title>Guest options</title>
 <para>
 </para>
 <sect2 id="vt.best.guest.allocation">
  <title>Over Allocation</title>
  <para>
   Allocation of resources for &vmguest; is a crucial point in VM
   administration. Each &vmguest; should be <emphasis>sized</emphasis> to be able 
   to run a certain amount of
   services, but over-allocating resources for &vmguest; may impact the host
   and all other &vmguest;s. If all &vmguest;s suddenly requested all their
   resources, the host won't be able to provide all of them, and this
   will impact the host's performance and will degrade all other services running
   on the host.
  </para>
  <para>
   CPU's Host <emphasis>components</emphasis> will be <emphasis>translated</emphasis> 
   as vCPU in the &vmguest;, but even if you have a
   multi-core CPU with Hyper threading, you should understand that a main CPU
   unit and a multi-core and Hyper threading doesn't provide the same computation capabilities:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     <emphasis>CPU processor</emphasis>: it describes the main CPU unit, it can be
     multi-core and Hyper threaded, and most of dedicated server have multi CPU
     processor on their motherboard.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>CPU core</emphasis>: a main CPU unit can provide more than one core, proximity of
     cores speed up computation process and reduce energy cost.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>CPU Hyper threading</emphasis>: this implementation is used to improve
     parrallelization of computations, but this is not efficient as a dedicated
     core.
    </para>
   </listitem>
  </itemizedlist>
 </sect2>
 

 <sect2 id="vt.best.guest.kbd">
  <title>Keyboard Layout</title>
  <para>
   Even if it is possible to specify the keyboard layout from a
   <command>qemu-system-ARCH</command> command it's recommended to do this
   configuration in the &libvirt; XML file. To change the keyboard layout while connecting to a remote
   &vmguest; using vnc you should edit the &vmguest; XML configuration file.
   The XML is located at
   <filename>/etc/libvirt/<replaceable>HYPERVISOR</replaceable></filename>. For
   example to add an "en-us" keymap add in the &lt;devices&gt; section:
  </para>
  <screen>&lt;graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/&gt;</screen>
  <para>
   Check the vncdisplay configuration and connect to your &vmguest;:
  </para>
  <screen>virsh vncdisplay sles12 127.0.0.1:0</screen>
 </sect2>

<!-- TODO
 <sect2 id="vt.best.guest.clock">
  <title>Clock Setting</title>
  <para>
   
   clock setting (common source)
   kvm clock and ntp   
  </para>
 </sect2>
 
 <sect2 id="vt.guest.guest.bio">
  <title>Bio-based</title>
  <para>
   bio-based driver on slow devices
  </para>
 </sect2>
-->
</sect1>


<sect1 id="vt.best.perf">
 <title>Performance</title>
 <para>
 </para>
 <sect2 id="vt.best.perf.general">
  <title>Virtio PV driver</title>
  <para>
   To increase &vmguest; performance it's recommended to use PV drivers, the host implementation is in userspace, so no driver is needed in the host.
   virtio is a virtualization standard, so the guest's device driver is aware that it is running in a virtual environment. Note that virtio is different, but architecturally similar to the &xen; paravirtualized device drivers (like VMDP in a Windows guest).
  </para>
  <itemizedlist>
   <listitem>
    <para>
     <emphasis>virtio_blk</emphasis>: the virtio block device for disk.
    </para>
    <warning>
     <title>&qemu; option</title>
     <para>
      The <option>-hd[ab]</option> for virtio disk won't work, you must use <option>-drive</option> instead.
     </para>
    </warning>
    <warning>
     <title>Disk Name Change</title>
     <para>
      Disk will show up as <option>/dev/vd[a-z][1-9]</option>, if you migrate from a non-virtio disk you need to change <option>root=</option> in GRUB config, and regenerate the <filename>initrd</filename> file or the system won't be able to boot.
     </para>
    </warning>
    <para>Example of a virtio disk definition:</para>
    <screen>&lt;disk type='....' device='disk'&gt;
    ....
    &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;</screen>
    <para>This is preferable to remove every disk block in the XML configuration containing <emphasis>&lt;address>&gt;</emphasis> because it will be re-generated automatically.</para>
   </listitem>
   <listitem>
    <para>
     <emphasis>virtio_net</emphasis>: the virtio network device. The kernel modules should be insmoded automatically in the guest at boot time. You need to start the service to get network available.
    </para>
    <screen>&lt;interface type='network'&gt;
    ...
    &lt;model type='virtio' /&gt;
&lt;/interface&gt;</screen>
   </listitem>
   <listitem>
    <para>
     <emphasis>virtio_balloon</emphasis>: a PV driver to give or take memory from a &vmguest;. It's controlled by <emphasis>currentMemory</emphasis> and <emphasis>memory</emphasis> option.
    </para>
    <screen>&lt;memory unit='KiB'&gt;16777216&lt;/memory&gt;
  &lt;currentMemory unit='KiB'&gt;1048576&lt;/currentMemory&gt;
  [...]
  &lt;devices&gt;
    &lt;memballoon model='virtio'/&gt;
 &lt;/devices&gt;</screen>
   </listitem>
  </itemizedlist>
  <para>
   Currently performance is much better when using a host kernel configured with <emphasis>CONFIG_HIGH_RES_TIMERS</emphasis>. Another option is use HPET/RTC and <option>-clock=</option> &qemu; option.
  </para>
 </sect2>

 <sect2>
  <title>Cirrus video driver</title>
  <para>
   To get 16bit color, high compatibility and better performance it's recommended to use the <emphasis>cirrus</emphasis> video driver.
  </para>
  <screen>&lt;video&gt;
  &lt;model type='cirrus' vram='9216' heads='1'/&gt;
&lt;/video&gt;</screen>
 </sect2>
 <sect2>
  <title>KSM and Page sharing</title>
  <para>
   Kernel Share Memory is a kernel module to increase memory density by merging equal anonymous pages on a system.
   This free memory on the system allow to run more &vmguest; on the same host. Running same Guest on a host generate a lot of common memory on the host. You can enable KSM with <command>echo 1 > /sys/kernel/mm/ksm/run</command>. One advantage of using KSM in a &vmguest; is that all guest memory is backed by host anonymous memory, so you can share <emphasis>pagecache</emphasis>, <emphasis>tmpfs</emphasis> or any kind of memory allocated on the guest.
  </para>
  <para>
   KSM is controlled by <emphasis>sysfs</emphasis>. You can check KSM's values in <filename>/sys/kernel/mm/ksm/</filename>.
  </para>
  <itemizedlist>
   <listitem>
    <para>
     <emphasis>pages_shared</emphasis>: how many shared pages with different content are being used (Read only).
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>pages_sharing</emphasis>: how many pages are being shared between your kvm guests (Read only).
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>pages_unshared</emphasis>: how many pages unique but repeatedly checked for merging (Read only).
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>pages_volatile</emphasis>: how many pages changing too fast to be placed in a tree (Read only).
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>full_scans</emphasis>: how many times all mergeable areas have been scanned (Read only).
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>sleep_millisecs</emphasis>: how many milliseconds <command>ksmd</command> should sleep before next scan. a low value will overuse the CPU, so you will loss CPU Time for other tasks, setting this to a value greater than <option>1000</option> is recommended.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>pages_to_scan</emphasis>: how many present pages to scan before ksmd goes to sleep. A big value will overuse the CPU, you can start with <option>1000</option>, and then adjust to an appropriate value based on KSM result (all read only data).
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>merge_across_nodes</emphasis>: by default the system merge page across NUMA node. set this option to <option>0</option> to disable this behavior.
    </para>
   </listitem>
  </itemizedlist>
  <note>
   <para>
    KSM should be used if you know that you will over-use your host system memory or you will run same instance of applications or &vmguest;. If this is not the case it's preferable to disable it. IE; in the XML configuration of the &vmguest; add:
   </para>
   <screen>&lt;memoryBacking&gt;
   &lt;nosharepages/&gt;
&lt;/memoryBacking&gt;</screen> 
  </note>
   <warning>
    <para>KSM can free up some memory on the host system, but the administrator should also reserve enough swap to avoid system running out of memory in case of the amount of share memory decrease (decrease of share memory will result on increasement of the physical memory).</para>
   </warning>
 </sect2>
 <sect2>
  <title>Swapping</title>
  <para>
   <emphasis>Swap</emphasis> is mostly used by the system to store under-used physical memory. Usage of <emphasis>swap</emphasis> is not recommended due to poor performance compare to physical memory, but to prevent the system to run out of memory setting up a swap with a minimum of 1/2 of the physical memory size is highly recommended.
  </para>
  <para>
   I/O Scheduler
   NFS storage
  </para>
 </sect2>
 
 <sect2 id="vt.best.perf.general.disable">
  <title>Disable Unused Tools and Devices</title>
  <para>
   You should minimize software and service available on the hosts. Moreover it's not
   recommended to mix different <emphasis>virtualization technologies</emphasis>, like KVM and
   Containers, on the same host, this will reduce resource, will increase
   security risk and software update queue. This lead to reduce the overall
   host availability, will degrade performance even if each resource for both
   technologies are well sized.
  </para>
  <para>
   Most of Operating System default installation configuration are not 
   optimized for VM usage. You should only install what you really need, and
   remove all other components in &vmguest;. 
  </para>
  <para>Windows Guest:</para>
  <itemizedlist>
   <listitem><para>Disable the screensaver</para></listitem>
   <listitem><para>Remove all graphical effects</para></listitem>
   <listitem><para>Disable indexing of hard drive disk if not
   necessary</para></listitem>
   <listitem><para>Check the list of started services and disable the one you
   don't need</para></listitem>
   <listitem><para>Check and remove all devices not necessary</para></listitem>
   <listitem><para>Disable system update if not needed or configure it to
   avoid any delay while rebooting or shutting down the host</para></listitem>
   <listitem><para>Check the Firewall rules</para></listitem>
   <listitem><para>Schedule appropriately backups and anti-virus programs</para></listitem>
   <listitem><para>Install <ulink url="https://www.suse.com/products/vmdriverpack/">VMDP</ulink>
   para-virtualized driver for best performance</para></listitem>
   <listitem><para>Check the operating System recommendation like in <ulink
   url="http://windows.microsoft.com/en-us/windows/optimize-windows-better-performance#optimize-windows-better-performance=windows-7">Microsoft
   Windows 7 better performance</ulink> web page</para></listitem>
  </itemizedlist>
  <para>Linux Guest:</para>
  <itemizedlist>
   <listitem><para>Remove Xorg start if not needed</para></listitem>
   <listitem><para>Check the list of started services and adjust
   accordingly</para></listitem>
   <listitem><para>Operating system needs specific kernel parameters for
   best performance, check the OS recommendations</para></listitem>
   <listitem><para>Restrict installation of software to a minimal
   fingerprint</para></listitem>
   <listitem><para>Optimize the scheduling of predictable tasks (system
   update, hard drive disk checking etc...)</para></listitem>
  </itemizedlist>
 </sect2>
 
 <sect2 id="vt.best.perf.general.mtype">
  <title>Updating guest machine type</title>
  <para>
   &qemu; machine types define some details of the architecture which are
   particularly relevant in the context of migration and save/restore, where all
   the details of the virtual machine ABI need to be carefully accounted for. As
   changes or improvements to &qemu; are made or certain types of fixes are done,
   new machine types are created which include those changes. Though the older
   machine types used for supported versions of &qemu; are still valid to use, the
   user would do well to try to move to the latest machine type supported by the
   current release, so as to be able to take advantage of all the changes
   represented in that machine type.
  </para>
  <para>
   Changing the guest's machine type for a
   Linux guest will mostly be transparent, whereas for Windows* guests, it is
   probably a good idea to take a snapshot or backup of the guest in case Windows*
   has issues with the changes it detects and subsequently the user decides to
   revert to the original machine type the guest was created with.
  </para>
 </sect2>
 
 <sect2 id="vt.best.perf.cpu">
  <title>CPU parameter</title>
  <para>
   You should avoid CPU over-commit. Unless you know exactly how many vCPU you
   require for your &vmguest; you should start with 1 vCPU per &vmguest;.
   Each vCPU should match one hardware processor or core. You should target a
   workload of 70% inside your VM (could be checked with a lot of tools like
   the <command>top</command>). If you allocate more processor than needed in
   the VM, this will add overhead, and will degrade cycle efficiency, the
   un-used vCPU will consume timer interrupts and will idle loop, then this
   will impact the &vmguest;, but also the host. To optimize the performance
   usage it's recommended to know if your applications are single threaded or not
   to avoid any over-allocation of vCPU.
  </para>
  <para>
   Scheduler
  </para>
  <itemizedlist>
   <listitem>
    <para>
     vCPU model and features: CPU model and topology can be specified for
     each &vmguest;. The vCPU definition could be very specific excluding
     some CPU features, listing exact one, etc... You can use predefined models available in
     <filename>cpu_map.xml</filename> file to exactly match your need.
     Event if could be interesting to declare a very specific vCPU for a
     &vmguest; you should keep in mind that's normalize vCPU model and
     features simplify migration among heterogeneous hosts. because change
     the vCPU type require that the &vmguest; is off, which is a constraint in a
     production environment. You should also consider that multiple sockets
     with a single core and thread generally give best performance.
    </para>
   </listitem>
   <listitem>
    <para>vCPU Pinning : it's a method to constrain vCPU threads to a NUMA
    node. The <emphasis>vcpupin</emphasis> element specifies which of host's physical CPUs the 
    domain vCPU will be pinned to. If this is omitted, and attribute <emphasis>cpuset</emphasis> of
    element <emphasis>vcpu</emphasis> is not specified, the vCPU is pinned to all the physical CPUs by
    default.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   For more information about vCPU configuration and tuning parameter 
   please refer to the <ulink
   url="https://libvirt.org/formatdomain.html#elementsCPU">libvirt</ulink> documentation.
  </para>
 </sect2>
 <sect2 id="vt.best.perf.numa">
  <title>NUMA</title>
  <para>
   Potentially huge impact on performance
   Consider host topology when sizing guests
   Avoid allocating VM memory across NUMA nodes
   Prevent vCPUs from floating across NUMA nodes
  </para>
 </sect2>

 <!-- TODO
      <sect2 id="vt.best.perf.mem">
      <title>Memory</title>
      <para>
      Hugepages
      </para>
      <para>
      HugeTLB
      </para>
      <para>
      Policy for allocation in NUMA topology:  strict,  interleave,  preferred
      </para>
      </sect2>
 -->
</sect1>


<sect1 id="vt.best.stor">
 <title>Storage and Filesystem</title>
 <para>
  plop
 </para>

<!--
 <sect2 id="vt.best.stor.general">
  <title>General</title>
  <para>
   Access CD/DVD -> storage pool
  </para>
  <para>
   deleting pool
  </para>
  <para>
   Brtfs and guest image
  </para>
  <para>
   SCSI Host Adapter name (avoid /dev/sdX)
  </para>
  <para>
   qemu direct access to host drives (-drive file=)
  </para>
  <para>
   QEMU Storage formats
  </para>
 </sect2>
 <sect2 id="vt.best.stor.blkvsimg">
  <title>BLK VS Images files</title>
  <para>
   you can use block devices or files as local storage devices within guest
   operating systems.
  </para>

  <para>
  Block devices:
  </para>
  <itemizedlist>
   <listitem><para>
    Better performance
   </para></listitem>
   <listitem><para>
    Use “standard” tools for administration/disk modification
    </para></listitem>
    <listitem><para>
     Accessible from host (pro and con)
    </para></listitem>
  </itemizedlist>
  <para>
   Image Files
  </para>
  <itemizedlist>
   <listitem><para>
    Easier system management
   </para></listitem>
   <listitem><para>
    Easily move, clone, backup domains
   </para></listitem>
   <listitem><para>
    Comprehensive toolkit (guestfs) for image manipulation
    </para></listitem>
    <listitem><para>
     Reduce overhead through sparse files
    </para></listitem>
    <listitem><para>
     Fully allocate for best performance
    </para></listitem>
  </itemizedlist>
 </sect2>
-->

 <sect2 id="vt.best.stor.imageformat">
  <title>Image Format</title>
  <para>
   Certain storage formats which &qemu; recognizes have their origins in other
   virtualization technologies. By recognizing these formats, &qemu; is able to
   leverage either data stores or entire guests which were originally targeted
   to run under these other virtualization technologies. Some of these formats
   are supported only in read-only mode, enabling either direct use of that read
   only data store in a &qemu; guest or conversion to a fully supported &qemu;
   storage format (using <command>qemu-img</command>) which could then be used in read/write mode.
   See &sle; <ulink url="https://www.suse.com/releasenotes/x86_64/SUSE-SLES/12/#fate-317891">Release Notes</ulink> to get the list of supported format.
  </para>
  <para>
   raw:
  </para>
  <itemizedlist>
   <listitem><para>
    Most common format
   </para></listitem>
   <listitem><para>
    Historically, best performance
   </para></listitem>
  </itemizedlist>
  <para>
   qcow2:
   </para>
   <itemizedlist>
    <listitem><para>
     Required for snapshot support in libvirt + tools
    </para></listitem>
    <listitem><para>
     Improved performance and stability
    </para></listitem>
   </itemizedlist>
   <para>
    qed: Next generation qcow
   </para>
   <para>
    vhd/vhdx: Also known as 'vpc'
   </para>
   <para>
    Suggest converting to raw or qcow2:
    <command>qemu-img convert -f vmdk -O qcow2 img.vmdk img.qcow2</command>
   </para>
 </sect2>

<!--
 <sect2 id="vt.best.stor.diskio">
  <title>Disk IO Modes</title>
  <table>
   <title>Notation Conventions</title>
   <tgroup cols="2">
    <colspec colnum="1" colname="1"/>
    <colspec colnum="2" colname="2"/>
    <thead>
     <row>
      <entry>
       <para>Mode</para>
      </entry>
      <entry>
       <para>Description</para>
      </entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>
       <para>Disk IO Modes
       </para>
      </entry>
      <entry>
       <para>Native</para>
      </entry>
     </row>
     <row>
      <entry>
       <para>kernel asynchronous IO</para>
      </entry>
      <entry>
       <para>threads</para>
      </entry>
     </row>
     <row>
      <entry>
       <para>host user-mode based threads</para>
      </entry>
      <entry>
       <para>default, 'threads' mode in SLES</para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </table>
 </sect2>
-->
</sect1>

<!--
<sect1 id="vt.best.snapsav">
 <title>Saving, migrating and Snapshoting</title>
 <para>
  Migration requirements/Recomendations
  save VM and start/boot (memory invalid)
  snapshot naming importance
  avoid qemu-img snapshot
  cache mode in live migration
  guestfs and live system
 </para>
</sect1>


<sect1 id="vt.best.security">
 <title>Security consideration</title>
 <para>
  Connection to guest: security policy
  Authentication for libvirtd and VNC need to be configured separately
  RNG and entropy
  &qemu; Guest Agent
  The VNC TLS (set at start)
 </para>
</sect1>

<sect1 id="vt.best.pcipass">
 <title>pcpipass</title>
 <para>
  Pci device (not online!, managed/unmanaged)
  howto check SR-IOV capabilities
 </para>
</sect1>
-->

<sect1 id="vt.best.net">
 <title>Network Tips</title>
 <para>
 </para>

 <sect2 id="vt.best.net.vnic">
  <title>Virtual NICs</title>
  <para>
   virtio-net (KVM) : multi-queue option
   vhost-net (KVM) : Default vNIC, best performance
   netbk (Xen) : kernel threads vs tasklets
  </para>
 </sect2>
 
 <sect2 id="vt.best.net.enic">
  <title>Emulated NICs</title>
  <para>
   e1000: Default and preferred emulated NIC
   rtl8139
  </para>
 </sect2>
 
 <sect2 id="vt.best.net.sharednic">
  <title>Shared Physical NICs</title>
  <para>
   SR-IOV: macvtap
   Physicial NICs : PCI pass-through
  </para>
 </sect2>

 <sect2 id="vt.best.general">
  <title>Network General</title>
  <para>
   use multi-network to avoid congestion
   admin, storage, migration ...
   use arp-filter to prevent arp flux

   same MTU in all devices to avoid fragmentation
   yast to configure bridge
   Network MAC address
   bridge configuration in bridge.conf file
   PCI pass-through Vfio to improve performance
  </para>
 </sect2>
</sect1>

<sect1 id="vt.best.debug">
 <title>Troubleshooting/Debugging</title>
 <para>
 </para>

 <sect2 id="vt.best.debug.xen">
  <title>Xen</title>
  <para>
  </para>
  <sect3 id="vt.best.debug.xen.log">
   <title>Xen Log Files</title>
   <para>
    libxl logs:
    <filename>/var/log/xen/*</filename>
    qemu-dm-domain.log, xl-domain.log
    bootloader.log, vm-install, xen-hotplug
    Process specific logs, often requiring debug log levels to be useful
    Some logs require 'set -x' to be added to /etc/xen/scripts/*

    libvirt logs:
    <filename>/var/log/libvirt/libxl</filename>
    libxl-driver.log
    domain.log
   </para>
  </sect3>
  <sect3 id="vt.best.debug.xen.hypervisor">
   <title>Daemon and Hypervisor Logs</title>
   <para>
    View systemd journal for specific units/daemons:
    <command>journalctl [--follow] –unit xencommons.service</command>
    <command>journalctl /usr/sbin/xenwatchdogd</command>
    xl dmesg
    Xen hypervisor logs
   </para>
  </sect3>
  
  <sect3 id="vt.best.debug.xen.loglevel">
   <title>Increasing Logging Levels</title>
   <para>
    Log levels are increased through xen parameters:
   </para>
    <screen>loglvl=all</screen>
    <para>
    Increased logging for Xen hypervisor
    </para>
    <screen>guest_loglvl=all</screen>
    <para>
     Increased logging for guest domain actions Grub2 config:
   
    Edit <filename>/etc/default/grub</filename>, then recreate <filename>grub.cfg</filename>:
    GRUB_CMDLINE_XEN_DEFAULT=”loglvl=all guest_loglvl=all”
    <command>grub2-mkconfig -o /boot/grub/grub.cfg</command>
   </para>
  </sect3>
 </sect2>

 <sect2 id="vt.best.debug.support">
  <title>Support</title>
  <para></para>
  <sect3 id="vt.best.debug.support.config">
   <title>Supportconfig and Virtualization</title>
   <para>
    Core files:
    basic-environment.txt
    Reports detected virtualization hypervisor
    Under some hypervisors (xen), subsequent general checks might be incomplete
    
    Hypervisor specific files:
    kvm.txt, xen.txt
    Both logs contain general information:
    RPM version/verification of important packages
    Kernel, hardware, network details
   </para>
  </sect3>
  
  <sect3 id="vt.best.debug.support.kvm">
   <title>kvm.txt</title>
   <para>
    libvirt details
    General libvirt details
    Libvirt daemon status
    KVM statistics
    virsh version, capabilities, nodeinfo, etc...
    
    Domain list and configurations
    Conf and log files
    <filename>/etc/libvirt/libvirtd.conf</filename>
    Last 500 lines from <filename>/var/log/libvirt/qemu/domain.log</filename>
   </para>
  </sect3>

  <sect3 id="vt.best.debug.support.xen">
   <title>xen.txt</title>
   <para>
    Daemon status
    xencommons, xendomains and xen-watchdog daemons
    grub/grub2 configuration (for xen.gz parameters)

    libvirt details
    Domain list and configurations
    
    xl details
    Domain list and configurations
    Conf and Log files
    <filename>/etc/xen/xl.conf</filename>, <filename>/etc/libvirt/libvirtd.conf</filename>
    Last 500 lines from <filename>/var/log/xen/*</filename>,
    <filename>/var/log/libvirt/libxl/*</filename> 
    Output of <command>xl dmesg</command> and <command>xl info</command>
   </para>
  </sect3>
 </sect2> 


 <sect2 id="vt.best.debug.advanced">
  <title>Advanced Debugging Options</title>
  <para>
   Serial console
  </para>
   <screen>GRUB_CMDLINE_XEN_DEFAULT=“loglvl=all guest_loglvl=all console=com1 com1=115200,8n1”</screen>
   <screen>GRUB_CMDLINE_LINUX_DEFAULT=“console=ttyS0,115200”</screen>
   <para>
    Debug keys
    <command>xl debug keys h; xl dmesg</command>
    <command>xl debug keys q; xl dmesg</command>
   Additional Xen debug tools:
   <command>xenstore-{ls,read,rm,watch,write}, xentrace, xentop</command>
   
   Capturing Guest Logs
   Capturing guest logs during triggered problem:
   Connect to domain:
   <command>virsh console domname</command>
   Execute problem command
   Capturing domain boot messages
   </para>
   <screen>xl create -c VM config file</screen>
   <screen>virsh create VM config file --console</screen>
 </sect2>
 <sect2 id="vt.best.trouble">
  <title>Troubleshooting Installations</title>
  <para>
   virt-manager and virt-install logs:
   Found in <filename>~/.cache/virt-manager</filename>

   Debugging virt-manager:
   <command>virt-manager --no-fork</command>
   Sends messages directly to screen and log file
  </para>
   <screen>LIBVIRT_DEBUG=1 virt-manager --no-fork</screen>
   <para>
   See libvirt messages in <filename>/var/log/messages</filename>

   Use <command>xl</command> to rule out libvirt layer
  </para>
 </sect2>
 <sect2 id="vt.best.trouble.libvirt"> 
  <title>Troubleshooting Libvirt</title>
  <para>
   Client side troubleshooting
  </para>
   <screen>LIBVIRT_DEBUG=1
   1: debug, 2: info, 3: warning, 4: error</screen>
   <para>
   Server side troubleshooting
   <filename>/etc/libvirt/libvirtd.conf</filename> (restart libvirtd after changes)
   log_level = 1
   log_output = “1:file:/var/log/libvirtd.log”
   log_filters = “1:qemu 3:remote”
  </para>
 </sect2>

 <sect2 id="vt.best.trouble.kernel">
  <title>Kernel Cores</title>
  <para>
   Host cores -vs- guest domain cores
   Host cores are enabled through Kdump YaST module
   For Xen dom0 cores, 'crashkernel=size@offset' should be added as a Xen hypervisor parameter

   Guest cores require:
   on_crash[action]on_crash tag
   Possible coredump actions are:
   coredump-restart     Dump core, then restart the VM
   coredump-destroy    Dump core, then terminate the VM
   Crashes are written to:
   <filename>/var/lib/libvirt/{libxl,qemu}/dump</filename>
   <filename>/var/lib/xen/dump</filename>  (if using xl).
  </para>
 </sect2>
 
 <sect2 id="vt.best.debug.other">
  <title>Other</title>
  <para>
   VGA trouble debug
  </para>
 </sect2>
</sect1>
<sect1>
 <title>References</title>
 <para>
 </para>
  <itemizedlist>
   <listitem><para>
    <ulink url="kernel.org/doc/ols/2009/ols2009-pages-19-28.pdf">Increasing memory density using KSM</ulink></para>
   </listitem>
   <listitem><para>
    <ulink url="http://www.linux-kvm.org/page/KSM">KVM KSM</ulink></para>
   </listitem>
   <listitem><para>
    <ulink url="https://www.kernel.org/doc/Documentation/vm/ksm.txt">KSM kernel documentation</ulink></para>
   </listitem>
   <listitem><para>
    <ulink url="http://www.espenbraastad.no/post/memory-ballooning/">Memory Ballooning</ulink></para>
   </listitem>
   <listitem><para>
    <ulink url="http://wiki.libvirt.org/page/Virtio">libvirt virtio</ulink></para>
   </listitem>
   <listitem><para>
    <ulink url=""></ulink></para>
   </listitem>
  </itemizedlist>
</sect1>
</article>
