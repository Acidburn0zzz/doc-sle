<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
[
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>

<article id="article.vt.best.practices" lang="en">
 <?suse-quickstart color="suse"?>
 <title>Virtualization Best Practices</title>
 <subtitle>&sle; &productname; &productnumber;</subtitle>
 <articleinfo>
  <productnumber>&productnumber;</productnumber><productname>&sle;
  &productname;</productname>
 </articleinfo>

<!--
we can write it based on on scenario?
Virtualization Capabilities:
Consolidation (hardware1+hardware2 -> hardware)
 Isolation
 Migration (hardware1 -> hardware2)
 Disaster recovery
 Dynamic load balancing
-->

<sect1 id="vt.best.intro">
 <title>Before Any Modification or Online Production</title>
 <para>
  The efficiency of virtualization environment depends on
  administration choice. This guide is provided as a reference for
  doing good choice in production environment. Nothing is "graved on
  stone", and different infra-structure can provide different
  result. Pre-experimentation is a key point to get a successfully
  virtualization environment.
 </para>
 
 <sect2 id="vt.best.intro.backup">
  <title>Backup First</title>
  <para>
   Playing with &vmguest; and the Host configuration can lead to data loss or
   unstable state. It's really important that you do backups of files,
   data, images etc.. before doing any change. Without backups you won't be able
   to easily restore the original state after a data loss or a
   miss-configuration.
  </para>
 </sect2>
 
 <sect2 id="vt.best.intro.acpi">
  <title>Do ACPI Testing</title>
  <para>
   The capabilities to change a &vmguest; state hevealy depends on the
   operating system. It's really important to test this features before
   any use of you &vmguest; in production. For example most of Linux OS
   disable this capabilities per default, so this require that you enable
   this operation (mostly through Policy Kit). 
  </para>
  <para>
   ACPI must be enabled in the guest for a graceful shutdown to work. 
   To check if ACPI is enabled, run:
  </para>
  <screen>virsh dumpxml <replaceable>VMNAME</replaceable> | grep acpi</screen>
  <para>
   If nothing is printed, ACPI is not enabled for your machine. Use <command>virsh edit</command>
   to add the following XML under &lt;domain&gt;:
  </para>
  <screen>&lt;features&gt;&lt;acpi/&gt;&lt;/features&gt;</screen>
  <para>
   If ACPI was enabled during a Windows Server 20XX guest installation,
   turning it on in the &vmguest;
   configuration alone is not sufficient. See the following articles for
   more information:
  </para>
  <simplelist>
   <member><ulink url="http://support.microsoft.com/kb/314088/EN-US/"/>
   </member>
   <member><ulink url="http://support.microsoft.com/?kbid=309283"/>
   </member>
  </simplelist>
  <para>
   A graceful shutdown is of course always possible from within the guest
   operating system, regardless of the &vmguest;'s configuration.
  </para>
 </sect2>

 <sect2 id="vt.best.intro.libvirt">
  <title>Prefer &libvirt; Framework</title>
  <para>
   In SUSE Linux Enterprise it is recommended to use the &libvirt; framework to
   do any operation on
   hosts, containers and &vmguest;. If you use other tools this may not appear
   or be propagated in you system. For example creating a system image by hand
   with <command>qemu-img <option>create</option> data.raw 10G</command> 
   will not be displayed in the <command>virt-manager</command> pool section. If you use a
   <command>qemu-system-arch</command> command this will no be visible under
   &libvirt;. So you should carefully used any other management tools and keep
   in mind their usage won't be probably reflected on other tools.
  </para>
 </sect2>
 <sect2 id="vt.best.intro.qemu">
  <title>qemu-system-i386 VS qemu-system-x86_64</title>
  <para>
   Just as a modern 64 bit x86 PC supports running a 32 bit OS as well as a 64
   bit OS, <command>qemu-system-x86_64</command> runs 32 bit OS's perfectly fine, and in fact
   usually provides better performance to 32 bit guests than <command>qemu-system-i386</command>,
   which provides a 32 bit guest environment only. Hence we recommend using
   <command>qemu-system-x86_64</command> over
   <command>qemu-system-i386</command> for all guest types. Where
   <command>qemu-system-i386</command>
   is known to perform better is in configurations which SUSE does not support.
  </para>
 </sect2>
</sect1>

<sect1 id="vt.best.guest">
 <title>Guest options</title>
 <para>
 </para>
 <sect2 id="vt.best.guest.allocation">
  <title>Over Allocation</title>
  <para>
   Allocation of resources for &vmguest; is a crucial point in VM
   administration. Each &vmguest; should be "sized" to be able to run a certain amount of
   services, but over-allocating resources for &vmguest; may impact the host
   and all other &vmguest;s. If all &vmguest;s suddenly requested all their
   resources, the host won't be able to provide all of them, and this
   will impact the host's performance and will degrade all other services running
   on the host.
  </para>
  <para>
   CPU's Host "components" will be "translated" as vCPU in the &vmguest;, but even if you have a
   multi-core CPU with Hyperthreading, you should understand that a main CPU
   unit, a multi-core and Hyperthreading doesn't provide the same computation capabilities:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     CPU processor: it describes the main CPU unit, it can be
     multi-core and Hyperthreaded, and most of dedicated server have multi CPU
     processor on their motherboard.
    </para>
   </listitem>
   <listitem>
    <para>
     CPU core: a main CPU unit can provide more than one core, proximity of
     cores speed up computation process and reduce energy cost.
    </para>
   </listitem>
   <listitem>
    <para>
     CPU Hyperthreading: this implementation is used to improve
     parrallelization of computations, but this is not efficient as a dedicated
     core.
    </para>
    <para>
     
    </para>
   </listitem>
  </itemizedlist>
 </sect2>
 

 <sect2 id="vt.best.guest.kbd">
  <title>Keyboard Layout</title>
  <para>
   To configure your keyboard layout while connecting to a remote
   &vmguest; using vnc you should edit the &vmguest; XML configuration file.
   The XML is located at
   <filename>/etc/libvirt/<replaceable>HYPERVISOR</replaceable></filename>. For
   example to add an "en-us" keymap add in the &lt;devices&gt; section:
  </para>
  <screen>&lt;graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/&gt;</screen>
  <para>
   Check the vncdisplay configuration and connect to your &vmguest;:
  </para>
  <screen>virsh vncdisplay sles12
127.0.0.1:0</screen>
 </sect2>
 
 <sect2 id="vt.best.guest.clock">
  <title>Clock Setting</title>
  <para>
   clock setting (common source)
   kvm clock and ntp   
  </para>
 </sect2>
 
 <sect2 id="vt.guest.guest.bio">
  <title>Bio-based</title>
  <para>
   bio-based driver on slow devices
  </para>
 </sect2>
</sect1>

<sect1 id="vt.best.perf">
 <title>Performance</title>
 <para>
 </para>
 <sect2 id="vt.best.perf.general">
  <title>General</title>
  <para>
   PV drivers should be used (virtio_blk, virtio_net, virtio_balloon)
   virtio-net recomendation (performance benefit)
   performance cirrus: 16bit color
   KSM and Page sharing
   Swapping
   I/O Scheduler
   NFS storage
  </para>
  
  <sect3 id="vt.best.perf.general.disable">
   <title>Disable Unused Tools and Devices</title>
   <para>
    You should minimize software on host, and service available. It's not
    recommended to mix different virtualization "technologies" (like KVM and
    Containers) on the same host, this will reduce resource, will increase
    security risk and software update queue. This will lead to reduce the overall host availability.
   </para>
   <para>
    Most of Operating System default installation configuration are not 
    optimized for VM usage. You should only install what you really need, and
    remove all other components in &vmguest;. 
   </para>
   <para>Windows Guest:</para>
   <itemizedlist>
    <listitem><para>Disable the screensaver</para></listitem>
    <listitem><para>Remove all graphical effects</para></listitem>
    <listitem><para>Disable indexing of hard drive disk if not
    necessary</para></listitem>
    <listitem><para>Check the list of started services and disable the one you
    don't need</para></listitem>
    <listitem><para>Check and remove all devices not necessary</para></listitem>
    <listitem><para>Disable system update if not needed or configure it to
    avoid any delay while rebooting or shutting down the host</para></listitem>
    <listitem><para>Check the Firewall rules</para></listitem>
    <listitem><para>Schedule appropriately backups and anti-virus programs</para></listitem>
    <listitem><para>Install <ulink url="https://www.suse.com/products/vmdriverpack/">VMDP</ulink>
    para-virtualized driver for best performance</para></listitem>
    <listitem><para>Check the operating System recommendation like in <ulink
    url="http://windows.microsoft.com/en-us/windows/optimize-windows-better-performance#optimize-windows-better-performance=windows-7">Micorsoft
    Windows 7 better performance</ulink> web page</para></listitem>
   </itemizedlist>
   <para>Linux Guest:</para>
   <itemizedlist>
    <listitem><para>Remove Xorg start if not needed</para></listitem>
    <listitem><para>Check the list of started services and adjust
    accordingly</para></listitem>
    <listitem><para>Operating system needs specific kernel parameters for
    best performance, check the OS recommendations</para></listitem>
    <listitem><para>Restrict installation of software to a minimal
    fingerprint</para></listitem>
    <listitem><para>Optimize the scheduling of predictable tasks (system
    update, hard drive disk checking etc...)</para></listitem>
   </itemizedlist>
  </sect3>
  
  <sect3 id="vt.best.perf.general.mtype">
   <title>Updating guest machine type</title>
   <para>
    QEMU machine types define some details of the architecture which are
    particularly relevant in the context of migration and save/restore, where all
    the details of the virtual machine ABI need to be carefully accounted for. As
    changes or improvements to QEMU are made or certain types of fixes are done,
    new machine types are created which include those changes. Though the older
    machine types used for supported versions of QEMU are still valid to use, the
    user would do well to try to move to the latest machine type supported by the
    current release, so as to be able to take advantage of all the changes
    represented in that machine type. Changing the guest's machine type for a
    Linux guest will mostly be transparent, whereas for Windows* guests, it is
    probably a good idea to take a snapshot or backup of the guest in case Windows*
    has issues with the changes it detects and subsequently the user decides to
    revert to the original machine type the guest was created with.
   </para>
  </sect3>
 </sect2>
 
 <sect2 id="vt.best.perf.cpu">
  <title>CPU parameter</title>
  <para>
   You should avoid CPU over-commit. Unless you know exactly how many vCPU you
   require for your &vmguest; you should start with 1 vCPU per &vmguest;.
   Each vCPU should match one hardware processor or core. You should target a
   workload of 70% inside your vm (could be checked with a lot of tools like
   the <command>top</command>). If you allocate more processor than needed in
   the VM, this will add overhead, and will degrade cycle efficiency, the
   un-used vCPU will consume timer interrups and will idle loop, then this
   will impact the &vmguest;, but also the host. To optimize the performance
   usage it's recommended to know if your applications are single threaded or not
   to avoid any over-allocation of vCPU.
  </para>
  <para>
   Scheduler
   vCPU model and features: Normalize to allow migration among heterogeneous hosts
   vCPU topology: Multiple sockets with a single core and thread generally give best performance
   vCPU Pinning : Constrain vCPU threads to a NUMA node
  </para>
 </sect2>
 <sect2 id="vt.best.perf.numa">
  <title>NUMA</title>
  <para>
   Potentially huge impact on performance
   Consider host topology when sizing guests
   Avoid allocating VM memory across NUMA nodes
   Prevent vCPUs from floating across NUMA nodes
  </para>
 </sect2>
 <sect2 id="vt.best.perf.mem">
  <title>Memory</title>
  <para>
   Hugepages
   HugeTLB
   Policy for allocation in NUMA topology:  strict,  interleave,  preferred
  </para>
 </sect2>
</sect1>


<sect1 id="vt.best.stor">
 <title>Storage and Filesystem</title>
 <para>
  
 </para>

 <sect2 id="vt.best.stor.general">
  <title>General</title>
  <para>
   Access CD/DVD -> storage pool
   deleting pool
   Brtfs and guest image
   SCSI Host Adapter name (avoid /dev/sdX)
   qemu direct access to host drives (-drive file=)
   QEMU Storage formats
  </para>
 </sect2>
 <sect2 id="vt.best.stor.blkvsimg">
  <title>BLK VS Images files</title>
  <para>
   Block devices
   Better performance
   Use “standard” tools for administration/disk modification
   Accessible from host (pro and con)

   Image Files
   Easier system management
   Easily move, clone, backup domains
   Comprehensive toolkit (guestfs) for image manipulation
   Reduce overhead through sparse files
   Fully allocate for best performance

  </para>
 </sect2>
 
 <sect2 id="vt.best.stor.imageformat">
  <title>Image Format</title>
  <para>
   Certain storage formats which QEMU recognizes have their origins in other
   virtualization technologies. By recognizing these formats, QEMU is able to
   leverage either data stores or entire guests which were originally targeted
   to run under these other virtualization technologies. Some of these formats
   are supported only in read-only mode, enabling either direct use of that read
   only data store in a QEMU guest or conversion to a fully supported QEMU
   storage format (using <command>qemu-img</command>) which could then be used in read/write mode.
   See &sle; <ulink url="https://www.suse.com/releasenotes/x86_64/SUSE-SLES/12/#fate-317891">Release Notes</ulink> to get the list of supported format.
  </para>
  <para>
   raw
   Most common format
   Historically, best performance
   qcow2
   Required for snapshot support in libvirt + tools
   Improved performance and stability
   qed
   Next generation qcow

   vhd/vhdx
   Also known as 'vpc'
   vmdk
   Performance
   Suggest converting to raw or qcow2
   <command>qemu-img convert -f vmdk -O qcow2 img.vmdk img.qcow2</command>
  </para>

 </sect2>

 <sect2 id="vt.best.stor.diskio">
  <title>Disk IO Modes</title>
  <para>
   Disk IO Modes
   native
   kernel asynchronous IO
   threads
   host user-mode based threads
   default
   'threads' mode in SLES
  </para>
 </sect2>
</sect1>


<sect1 id="vt.best.snapsav">
 <title>Saving, migrating and Snapshoting</title>
 <para>
  Migration requirements/Recomendations
  save VM and start/boot (memory invalid)
  snapshot naming importance
  avoid qemu-img snapshot
  cache mode in live migration
  guestfs and live system
 </para>
</sect1>


<sect1 id="vt.best.security">
 <title>Security consideration</title>
 <para>
  Connection to guest: security policy
  Authentication for libvirtd and VNC need to be configured separately
  RNG and entropy
  QEMU Guest Agent
  The VNC TLS (set at start)
 </para>
</sect1>


<sect1 id="vt.best.pcipass">
 <title>pcpipass</title>
 <para>
  Pci device (not online!, managed/unmanaged)
  howto check SR-IOV capabilities
 </para>
</sect1>

<sect1 id="vt.best.net">
 <title>Network Tips</title>
 <para>
 </para>

 <sect2 id="vt.best.net.vnic">
  <title>Virtual NICs</title>
  <para>
   virtio-net (KVM) : multi-queue option
   vhost-net (KVM) : Default vNIC, best performance
   netbk (Xen) : kernel threads vs tasklets
  </para>
 </sect2>
 
 <sect2 id="vt.best.net.enic">
  <title>Emulated NICs</title>
  <para>
   e1000: Default and preferred emulated NIC
   rtl8139
  </para>
 </sect2>
 
 <sect2 id="vt.best.net.sharednic">
  <title>Shared Physical NICs</title>
  <para>
   SR-IOV: macvtap
   Physicial NICs : PCI passthrough
  </para>
 </sect2>

 <sect2 id="vt.best.general">
  <title>Network General</title>
  <para>
   use multi-network to avoid congestion
   admin, storage, migration ...
   use arp-filter to prevent arp flux

   same MTU in all devices to avoid fragmentation
   yast to configure bridge
   Network MAC address
   bridge configuration in bridge.conf file
   PCI passthrough Vfio to improve performance
  </para>
 </sect2>
</sect1>

<sect1 id="vt.best.debug">
 <title>Troubleshooting/Debuging</title>
 <para>
 </para>

 <sect2 id="vt.best.debug.xen">
  <title>Xen</title>
  <para>
  </para>
  <sect3 id="vt.best.debug.xen.log">
   <title>Xen Log Files</title>
   <para>
    libxl logs:
    /var/log/xen/*
    qemu-dm-domain.log, xl-domain.log
    bootloader.log, vm-install, xen-hotplug
    Process specific logs, often requiring debug log levels to be useful
    Some logs require 'set -x' to be added to /etc/xen/scripts/*

    libvirt logs:
    /var/log/libvirt/libxl
    libxl-driver.log
    domain.log
   </para>
  </sect3>
  <sect3 id="vt.best.debug.xen.hypervisor">
   <title>Daemon and Hypervisor Logs</title>
   <para>
    View systemd journal for specific units/daemons:
    <command>journalctl [--follow] –unit xencommons.service</command>
    <command>journalctl /usr/sbin/xenwatchdogd</command>
    xl dmesg
    Xen hypervisor logs
   </para>
  </sect3>
  
  <sect3 id="vt.best.debug.xen.loglevel">
   <title>Increasing Logging Levels</title>
   <para>
    Log levels are increased through xen parameters:
   </para>
    <screen>loglvl=all</screen>
    <para>
    Increased logging for Xen hypervisor
    </para>
    <screen>guest_loglvl=all</screen>
    <para>
     Increased logging for guest domain actions Grub2 config:
   
    Edit <filename>/etc/default/grub</filename>, then recreate <filename>grub.cfg</filename>:
    GRUB_CMDLINE_XEN_DEFAULT=”loglvl=all guest_loglvl=all”
    <command>grub2-mkconfig -o /boot/grub/grub.cfg</command>
   </para>
  </sect3>
 </sect2>

 <sect2 id="vt.best.debug.support">
  <title>Support</title>
  <para></para>
  <sect3 id="vt.best.debug.support.config">
   <title>Supportconfig and Virtualization</title>
   <para>
    Core files:
    basic-environment.txt
    Reports detected virtualization hypervisor
    Under some hypervisors (xen), subsequent general checks might be incomplete
    
    Hypervisor specific files:
    kvm.txt, xen.txt
    Both logs contain general information:
    RPM version/verification of important packages
    Kernel, hardware, network details
   </para>
  </sect3>
  
  <sect3 id="vt.best.debug.support.kvm">
   <title>kvm.txt</title>
   <para>
    libvirt details
    General libvirt details
    Libvirt daemon status
    KVM statistics
    virsh version, capabilities, nodeinfo, etc...
    
    Domain list and configurations
    Conf and log files
    <filename>/etc/libvirt/libvirtd.conf</filename>
    Last 500 lines from <filename>/var/log/libvirt/qemu/domain.log</filename>
   </para>
  </sect3>

  <sect3 id="vt.best.debug.support.xen">
   <title>xen.txt</title>
   <para>
    Daemon status
    xencommons, xendomains and xen-watchdog daemons
    grub/grub2 configuration (for xen.gz parameters)

    libvirt details
    Domain list and configurations
    
    xl details
    Domain list and configurations
    Conf and Log files
    <filename>/etc/xen/xl.conf</filename>, <filename>/etc/libvirt/libvirtd.conf</filename>
    Last 500 lines from <filename>/var/log/xen/*</filename>,
    <filename>/var/log/libvirt/libxl/*</filename> 
    Output of <command>xl dmesg</command> and <command>xl info</command>
   </para>
  </sect3>
 </sect2> 


 <sect2 id="vt.best.debug.advanced">
  <title>Advanced Debugging Options</title>
  <para>
   Serial console
  </para>
   <screen>GRUB_CMDLINE_XEN_DEFAULT=“loglvl=all guest_loglvl=all console=com1 com1=115200,8n1”</screen>
   <screen>GRUB_CMDLINE_LINUX_DEFAULT=“console=ttyS0,115200”</screen>
   <para>
    Debug keys
    <command>xl debug keys h; xl dmesg</command>
    <command>xl debug keys q; xl dmesg</command>
   Additional Xen debug tools:
   <command>xenstore-{ls,read,rm,watch,write}, xentrace, xentop</command>
   
   Capturing Guest Logs
   Capturing guest logs during triggered problem:
   Connect to domain:
   <command>virsh console domname</command>
   Execute problem command
   Capturing domain boot messages
   </para>
   <screen>xl create -c VM config file</screen>
   <screen>virsh create VM config file --console</screen>
 </sect2>
 <sect2 id="vt.best.trouble">
  <title>Troubleshooting Installations</title>
  <para>
   virt-manager and virt-install logs:
   Found in <filename>~/.cache/virt-manager</filename>

   Debugging virt-manager:
   <command>virt-manager --no-fork</command>
   Sends messages directly to screen and log file
  </para>
   <screen>LIBVIRT_DEBUG=1 virt-manager --no-fork</screen>
   <para>
   See libvirt messages in <filename>/var/log/messages</filename>

   Use <command>xl</command> to rule out libvirt layer
  </para>
 </sect2>
 <sect2 id="vt.best.trouble.libvirt"> 
  <title>Troubleshooting Libvirt</title>
  <para>
   Client side troubleshooting
  </para>
   <screen>LIBVIRT_DEBUG=1
   1: debug, 2: info, 3: warning, 4: error</screen>
   <para>
   Server side troubleshooting
   <filename>/etc/libvirt/libvirtd.conf</filename> (restart libvirtd after changes)
   log_level = 1
   log_output = “1:file:/var/log/libvirtd.log”
   log_filters = “1:qemu 3:remote”
  </para>
 </sect2>

 <sect2 id="vt.best.trouble.kernel">
  <title>Kernel Cores</title>
  <para>
   Host cores -vs- guest domain cores
   Host cores are enabled through Kdump YaST module
   For Xen dom0 cores, 'crashkernel=size@offset' should be added as a Xen hypervisor parameter

   Guest cores require:
   on_crash[action]on_crash tag
   Possible coredump actions are:
   coredump-restart     Dump core, then restart the VM
   coredump-destroy    Dump core, then terminate the VM
   Crashes are written to:
   <filename>/var/lib/libvirt/{libxl,qemu}/dump</filename>
   <filename>/var/lib/xen/dump</filename>  (if using xl).
  </para>
 </sect2>
 
 <sect2 id="vt.best.debug.other">
  <title>Other</title>
  <para>
   VGA trouble debug
  </para>
 </sect2>
</sect1>
</article>
