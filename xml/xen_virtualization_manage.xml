<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE chapter PUBLIC "-//Novell//DTD NovDoc XML V1.0//EN" "novdocx.dtd"
  [
  <!ENTITY % NOVDOC.DEACTIVATE.IDREF "INCLUDE">
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter id="cha.xen.manage">
 <title>Managing a Virtualization Environment</title>
 <para>
  Apart from using the recommended &libvirt; library
  (<xref
 linkend="part.virt.libvirt"/>), you can manage &xen; guest domains
  with the <command>xl</command> tool from the command line.
 </para>
 <sect1 id="sec.xen.manage.xl">
  <title>XL - &xen; Management Tool</title>

  <para>
   The <command>xl</command> program is a tool for managing &xen; guest
   domains. It is based on the LibXenlight library, and can be used for
   general domain managing, such as domain creation, listing, pausing, or
   shutting down. In most cases, you need to be &rootuser; to execute
   <command>xl</command> commands.
  </para>

  <note>
   <para>
    <command>xl</command> can only manage running guest domains specified by
    their configuration file. If a guest domain is not running, you cannot
    manage it with <command>xl</command>.
   </para>
  </note>

  <tip>
   <para>
    To allow users to continue to have managed guest domains in the way the
    obsolete <command>xm</command> command allowed, we now recommend using
    &libvirt;'s <command>virsh</command> and <command>virt-manager</command>
    tools. For more information, see <xref linkend="part.virt.libvirt"/>.
   </para>
  </tip>

  <para>
   <command>xl</command> operations rely upon
   <systemitem>xenstored</systemitem> and
   <systemitem>xenconsoled</systemitem> services. Make sure you start
  </para>

<screen>systemctl start xencommons.service</screen>

  <para>
   at boot time to initialize all the daemons required by
   <command>xl</command>.
  </para>

  <tip>
   <title>Setup a <literal>xenbr0</literal> Network Bridge in the Host Domain</title>
   <para>
    In the most common network configuration, you need to setup a bridge in
    the host domain named <literal>xenbr0</literal> in order to have a
    working network for the guest domains.
   </para>
  </tip>

  <para>
   The basic structure of every <command>xl</command> command is following:
  </para>

<screen>xl &lt;subcommand> [options] domain_id</screen>

  <para>
   where &lt;subcommand> is the xl command to run, domain_id is the ID
   number assigned to a domain or the name of the virtual machine, and
   <command>OPTIONS</command> indicates subcommand-specific options.
  </para>

  <para>
   For a complete list of the available <command>xl</command> subcommands,
   run <command>xl help</command>. For each command, there is a more
   detailed help available that is obtained with the extra parameter
   <systemitem>--help</systemitem>. More information about the respective
   subcommands is available in the manual page of <command>xl</command>.
  </para>

  <para>
   For example, the <command>xl list --help</command> displays all options
   that are available to the list command. As an example, the <command>xl
   list</command> command displays the status of all virtual machines.
  </para>

<screen>
# xm list
Name                                 ID    Mem VCPUs        State   Time(s)
Domain-0                              0    457     2       r-----   2712.9
sles12                                7    512     1       -b----     16.3
opensuse                                   512     1                  12.9
</screen>

  <para>
   The <guimenu>State</guimenu> information tells if a machine is running,
   and in which state it is. The most common flags are <literal>r</literal>
   (running) and b (blocked) where blocked means it is either waiting for
   IO, or sleeping because there is nothing to do. For more details about
   the state flags, see <command>man 1 xl</command>.
  </para>

  <para>
   Other useful <command>xl</command> commands include:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>xl create</command> creates a virtual machine from a give
     configuration file
    </para>
   </listitem>
   <listitem>
    <para>
     <command>xl reboot </command>reboots a virtual machine
    </para>
   </listitem>
   <listitem>
    <para>
     <command>xl destroy</command> immediately terminates a virtual machine
    </para>
   </listitem>
   <listitem>
    <para>
     <command>xl block-list</command> displays all virtual block devices
     attached to a virtual machine
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 id="sec.xen.manage.autostart">
  <title>Automatic Starting of Domains</title>

  <para>
   If you need automatic starting of domains at boot time, or after a crash,
   the Xend must be configured to execute the desired behavior. There are
   five different situations that need to be handled.
  </para>

  <variablelist>
   <varlistentry>
    <term>After boot of the Hypervisor</term>
    <listitem>
     <para>
      Set the Xend variable <literal>on_xend_start</literal> to the desired
      value. For more details, see <xref linkend="domain.on_xend_start"/>.
      Example:
     </para>
<screen>(on_xend_start start)</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>When shutting down Xend</term>
    <listitem>
     <para>
      Xend can tell the &vmguest; system to shut down. However, it does not
      to check if the guest was stopped when doing a system shutdown of
      &dom0;. Thus, it is not recommended to rely on this feature. Example:
     </para>
<screen>(on_xend_stop shutdown)</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>When rebooting the &vmguest;</term>
    <listitem>
     <para>
      Xend has control about what to do when a &vmguest; does a reboot. By
      default, it is restart the guest:
     </para>
<screen>(on_reboot restart)</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>During poweroff of a &vmguest;</term>
    <listitem>
     <para>
      When a guest is shut off, the Xend by default destroys the guest
      without shutting it down.
     </para>
<screen>(on_poweroff destroy)</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>After a crash of the &vmguest;</term>
    <listitem>
     <para>
      After a &vmguest; crashes, the Xend can restart the guest. This is
      also the default:
     </para>
<screen>(on_crash restart)</screen>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
<!-- 2009-05-13 bg: conversion of images to qcow images is broken
                     see bnc 503332
 <sect1 id="sec.xen.manage.snapshots">
  <title>Creating Snapshots of &vmguest; Systems</title>
  <para>
  </para>
 </sect1>
 -->
 <sect1 id="sec.xen.manage.migrate">
  <title>Migrating &xen; &vmguest; Systems</title>

  <para>
   With &xen; it is possible to migrate a &vmguest; system from one &vmhost;
   to another with almost no service interruption. This could be used for
   example to move a busy &vmguest; to a &vmhost; that has stronger hardware
   or is not yet loaded. Or, if a service of a &vmhost; is required, all
   &vmguest; systems running on this machine can be migrated to other
   machines in order to avoid interruption of service. These are only two
   examples, many more reasons may apply to your personal situation.
  </para>

  <para>
   Before starting, some preliminary considerations regarding the &vmhost;
   should be taken:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     All &vmhost; systems should use a similar CPU. The frequency is not so
     important, but they should be using the same CPU family. To get more
     information about the used CPU, see <command>cat
     /proc/cpuinfo</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     All resources that are used by a specific guest system must be
     available on all involved &vmhost; systems. This means, the network
     bridges must be in the same subnet, and all used block devices must
     exist on both &vmhost; systems.
    </para>
   </listitem>
   <listitem>
    <para>
     Using special features like <literal>&pciback;</literal> may be
     problematic. Do not implement these when deploying for an environment
     that should migrate &vmguest; systems between different &vmhost;
     systems.
    </para>
   </listitem>
   <listitem>
    <para>
     For fast migrations, a fast network is mandatory. If possible, use GB
     Ethernet and fast Switches. Deploying VLAN might also help avoiding
     collisions.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 id="sec.xen.manage.migrate.xend">
   <title>Configuring Xend for Migrations</title>
   <para>
    To prepare a &vmhost; system for migrating, edit the configuration file
    <filename>/etc/xen/xend-config.sxp</filename>. Search for the following
    lines:
   </para>
<screen>
#(xend-relocation-server no)
#(xend-relocation-port 8002)
(xend-relocation-hosts-allow '^localhost$ ^localhost\\.localdomain$')
  </screen>
   <para>
    Change the lines to match the following strings:
   </para>
<screen>
(xend-relocation-server yes)
(xend-relocation-port 8002)
(xend-relocation-hosts-allow '^localhost$ ^localhost\\.localdomain$ \
   ^&lt;relocation_host&gt;')
  </screen>
   <para>
    These changes must be done on all &vmhost; systems that should
    participate in migrating guests.
   </para>
  </sect2>

  <sect2 id="sec.xen.manage.migrate.block">
   <title>Preparing Block Devices for Migrations</title>
   <para>
    The block devices needed by the &vmguest; system must be available on
    all involved &vmhost; systems. This is done by implementing some kind of
    shared storage that serves as container for the root file system of the
    migrated &vmguest; system. Common possibilities include:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>iSCSI</literal> can be set up to give access to the same
      block devices from different systems at the same time. For more
      information about iSCSI, see
      <ulink url="http://www.suse.com/doc/sles11/stor_admin/data/cha_inst_system_iscsi.html"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>NFS</literal> is a widely used root file system that can
      easily be accessed from different locations.
<!-- For more information, see <xref
      linkend="cha.nfs"/>. -->
     </para>
    </listitem>
    <listitem>
     <remark>mdejmek: check whether DRBD is used for the first time in this text; explain it??</remark>
     <para>
      <literal>DRBD</literal> can be used, if only two &vmhost; systems are
      involved. This gives some extra data security, because the used data
      is mirrored over the network. For more information, see
      <ulink
      url="http://www.suse.com/doc/sles11/book_sleha/data/cha_ha_drbd.html"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>SCSI</literal> can also be used, if the available hardware
      permits shared access to the same disks.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>NPIV</literal> is a special mode to use fibre channel disks.
      However, in this case all migration hosts must be attached to the same
      fibre channel switch. For more information about NPIV, see
      <xref
      linkend="sec.xen.config.disk"/>. Commonly, this works if
      the fibre channel environment supports 4 GBit or faster connections.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 id="sec.xen.manage.migrate.xm">
   <title>Migrating &vmguest; Systems</title>
   <para>
    The actual migration of the &vmguest; system is done with the command:
   </para>
<screen>xm migrate --live &lt;domain_name&gt; &lt;host&gt;</screen>
   <para>
    The option <systemitem>--live</systemitem> must be set to migrate a
    system that is currently running.
   </para>
   <para>
    The speed of the migration depends on how fast the memory print can be
    saved to disk, sent to the new &vmhost; and loaded there. This means,
    that small &vmguest; systems can be migrated faster than big systems
    with a lot of memory.
   </para>
  </sect2>
 </sect1>
</chapter>
